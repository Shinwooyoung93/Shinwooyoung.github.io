---
layout: post
title:  "FDA report"
date:   2019-06-21
use_math: true
tags:
 - R
 - korean
 - Report
---

# 배달데이터를 이용한 Functional Linear Model 적합

## 1. Data description

SKT Big data hub( https://www.bigdatahub.co.kr )에서 2018년의 서울시 지역별 배달 통화 건수를 사용하였다.

![](/assets/FDA/0_1.png)

이를 통해 2018년 서울시 지역별 인구수를 예측하는 것이 최종 목표이므로 상주인구와 관련이 적은 주간(낮 시간)의 데이터를 제외하였다.

또한 구별 전체 전화 횟수를 고려하기 위해 업종과 시 $\cdot$ 도 $\cdot$ 읍면동의 변수를 제거하였다. 따라서 사용할 데이터의 형태는 다음과 같다.

![](/assets/FDA/0_2.png)

그리고 우리가 예측하고자 하는 대상인 2018년 4분기의 서울시 지역별 데이터를 서울 열린 데이터 광장( https://data.seoul.go.kr )에서 가져왔다.

![](/assets/FDA/0_3.png)

우선 일자별(daily) 데이터를 탐색하였는데 우리가 사용하고자 하는 Functional Linear Model에서 사용될 Fourier basis로는 아래의 큰 변동성을 포착하기 어려울 것이라고 판단했다.

![](/assets/FDA/0_4.png)

따라서 변동성을 줄일 수 있는 주별(weekly)데이터를 사용하였다.

![](/assets/FDA/0_5.png)

최종적으로 서울시 25개 구의 53개 주(week)와 상주인구 수의 데이터를 이용해 Functional Linear Model을 적합해 인구수를 예측한다.

## 2. Modeling

### 2-1. Naive approach with none-penalty

$$
y_i = \alpha_0 + \int x_i(t)\beta(t)dt + \epsilon_i
$$

```r
rm(list = ls())
set.seed(1)
library(fda)
setwd("C:\\Users\\shin\\Desktop\\대학원\\고려대학교\\이론통계특수연구_구자용\\FDA_PR")
raw.data <- read.csv("week.csv")

name <- raw.data$province
y <- as.numeric(raw.data$population)
y <- log(y)
names(y) <- name
n.basis <- 23
basis.range <- c(0, 53)
X <- t(raw.data[,-c(1,2)])
colnames(X) <- name
rownames(X) <- 1:53

tempbasis <- create.fourier.basis(basis.range, 51)
tempSmooth<- smooth.basis(day.5[1:53], X, tempbasis)
tempfd <- tempSmooth$fd
tempfd$fdnames$time <- as.numeric(tempfd$fdnames$time)
templist <- vector("list", 2)
templist[[1]] <- rep(1,25)
templist[[2]] <- tempfd

conbasis <- create.constant.basis(basis.range)
betabasis<- create.fourier.basis(basis.range, n.basis)
betalist <- vector("list",2)
betalist[[1]] <- conbasis
betalist[[2]] <- betabasis
fRegressList  <- fRegress(y, templist, betalist)

betaestlist <- fRegressList$betaestlist
tempbetafd  <- betaestlist[[2]]$fd
plot(tempbetafd, xlab = "Week", ylab = "Beta for calling",
     main = paste("Beta function, basis =", n.basis))
```

![](/assets/FDA/0_5_0.png)

추정된 $\alpha_0$는 

```r
coef(betaestlist[[1]])
```
```r
##           [,1]
## [1,] 0.2516832
```

추정된 $\beta(t)$는
```r
coef(betaestlist[[2]])
```
```r
##                [,1]
##  [1,] -1.457559e-05
##  [2,] -1.724826e-04
##  [3,] -5.154319e-04
##  [4,]  1.535949e-04
##  [5,] -8.837605e-05
##  [6,]  2.249627e-04
##  [7,] -6.237810e-04
##  [8,] -8.736664e-06
##  [9,] -1.240187e-03
## [10,]  2.478494e-03
## [11,]  2.352714e-03
## [12,] -3.138077e-03
## [13,] -9.759771e-04
## [14,] -1.363507e-03
## [15,]  7.992491e-04
## [16,] -3.982134e-03
## [17,] -8.818847e-04
## [18,]  1.097410e-03
## [19,] -3.646612e-04
## [20,]  2.659182e-03
## [21,]  6.004545e-05
## [22,]  1.220189e-03
## [23,] -2.430108e-03
```

그에 따른 예측값과 $R^2$는
```r
pred1 <- fRegressList$yhatfdobj
resid1 <- y - pred1
SSE1 <- sum(resid1^2)
SSE0 <- sum((y - mean(y))^2)

RSQ1 = (SSE0-SSE1)/SSE0
Fratio1 = ((SSE0-SSE1)/(n.basis))/(SSE1/(25 - n.basis - 1))

RSQ1
```
```r
## [1] 0.9850475
```
```r
Fratio1
```
```r
## [1] 2.864281
```

![](/assets/FDA/0_5_1.png)

이러한 과정을 다양한 홀수 basis마다 실시한 결과가 아래와 같다.

![](/assets/FDA/0_6.png)

$\beta(t)$는 basis의 갯수가 늘어나면 늘어날수록, 과적합되는 경향을 보인다. 아래의 그림을 살펴보면

![](/assets/FDA/0_7.png)

$R^2$가 증가하면 증가할 수록 CV error가 증가하는 것을 확인할 수 있다. CV error란

$$
\text{CV}_{lambda} = \sum_{i = 1}^N\left[y_i - \alpha_{\lambda}^{(-i)} - \int x_i(t)\beta_{\lambda}^{(-i)}dt\right]^2
$$

다음과 같이 계산되며 과적합 때문에 값이 증가하는 것을 확인할 수 있다. 따라서 penalty를 부여한 방법을 필요로 한다.

## 2-2. Choosing smoothing parameters

```r
lam <- seq(5, 15, 0.5)
nlam <- length(lam)
SSE.CV <- matrix(0,nlam,1)
for(ilam in 1:nlam){
  Lcoef <- c(0, (2*pi/53)^2, 0)
  harmaccelLfd <- vec2Lfd(Lcoef, basis.range)
  lambda <- 10^lam[ilam]
  betalisti <- betalist
  betafdPar <- fdPar(betabasis, harmaccelLfd, lambda)
  betalisti[[2]] <- betafdPar
  fRegi <- fRegress.CV(y, templist, betalisti)
  SSE.CV[ilam] <- fRegi$SSE.CV
}
plot(lam, SSE.CV, main = "Cross-validation score", 
     xlab = "log smoothing parameter lambda", ylab = "Cross-validation scroe", type = "b")
```

![](/assets/FDA/0_8.png)

```r
min.score <- lam[which.min(SSE.CV)]
min.score
```
```r
## [1] 11
```

## 2-3. Coefficient $\beta(t)$ Estimate Using a Roughness Penalty

$$
\text{PENSSE}_{\lambda}(\alpha_0, \beta) = \sum_{i = 1}^N\left[y_i - \alpha_0 - \int x_i(t)\beta(t)dt\right]^2 + \lambda \int[L\beta(t)]^2dt
$$

따라서 이를 이용해 penalty를 사용한 계수를 추정한 결과는 다음의 수식으로 계산된다.

$$
\hat{b} = (Z^tZ + R(\lambda))^{-1}Z^ty
$$

```r
Lcoef <- c(0, (2*pi/53)^2, 0)
harmaccelLfd <- vec2Lfd(Lcoef, basis.range)
betabasis <- create.fourier.basis(basis.range, n.basis)
lambda <- 10^11
betafdPar <- fdPar(betabasis, harmaccelLfd, lambda)
betalist[[2]] <- betafdPar

rough.fit <- fRegress(y, templist, betalist)
betaestlist2<- rough.fit$betaestlist
pred2 <- rough.fit$yhatfdobj
betafd  <- betaestlist2[[2]]$fd

SSE1.2 <- sum((y-pred2)^2)
RSQ2 <- (SSE0 - SSE1.2)/SSE0;RSQ2
```
```r
## [1] 0.6041613
```
```r
plot(betafd, main = paste("Use roughness penalty, basis =", n.basis), 
     ylab = "Beta for calling", xlab = "Week")
```

![](/assets/FDA/0_9.png)

```r
plot(pred2, y, main = "Use roughness penalty")
```

![](/assets/FDA/0_10.png)

과적합 문제가 해결된 것으로 보이며 그래프의 패턴이 매우 단순해진 것을 확인할 수 있다.

## 3. Summary

이러한 과정을 실시해 basis의 갯수별로 비교한 결과

![](/assets/FDA/0_11.png)
![](/assets/FDA/0_12.png)

다음의 그래프들을 얻을 수 있었는데, penalty가 들어간 $\beta(t)$의 그래프는 대부분이 일치하는 것을 확인할 수 있었다.

이는 본 데이터에 `LOESS`를 이용해 적합했을 경우 단순한 패턴을 지니기 때문으로 보인다.

![](/assets/FDA/0_14.png)

따라서 penalty를 준 11개의 basis를 갖는 모델을 최종모델로 설정하며

![](/assets/FDA/0_13.png)

$\beta(t)$가 0을 포함하지 않는 유의한 구간이 3개 존재하는 것을 확인하였다.

