---
layout: post
title:  "KPCA"
date:   2019-05-01
use_math: true
tags:
 - Python
 - korean
 - Research
---

# 1. Introduction

다음의 예시를 살펴보자

```python
%reset -f

from sklearn.datasets import make_circles
import matplotlib.pyplot as plt

X, y = make_circles(n_samples = 200, random_state = 1, noise = 0.1, factor = 0.2)
plt.scatter(X[y == 0, 0], X[y == 0, 1], color='red', marker='^', alpha=0.5)
plt.scatter(X[y == 1, 0], X[y == 1, 1], color='blue', marker='o', alpha=0.5)
plt.tight_layout()
plt.title('True plot', size = 15)
plt.show()
```

![](/asstets/KPCA/2.png)

위의 2차원 변수에 대해 PCA를 실시보자.

```python
from sklearn.decomposition import PCA
import numpy as np

pca = PCA(n_components = 2)
X_pca = pca.fit_transform(X)
fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (10, 5))
ax[0].scatter(X_pca[y == 0, 0], X_pca[y == 0, 1], color = 'red', marker = '^', alpha = 0.5)
ax[0].scatter(X_pca[y == 1, 0], X_pca[y == 1, 1], color = 'blue', marker = 'o', alpha = 0.5)
ax[1].scatter(X_pca[y == 0, 0], np.zeros((100, 1)) + 0.02, color = 'red', marker = '^', alpha = 0.5)
ax[1].scatter(X_pca[y == 1, 0], np.zeros((100, 1)) - 0.02, color = 'blue', marker = 'o', alpha = 0.5)
ax[0].set_xlabel('PC1')
ax[0].set_ylabel('PC2')
ax[0].set_title('2-dimensional PCA', size = 15)
ax[1].set_ylim([-1, 1])
ax[1].set_yticks([])
ax[1].set_xlabel('PC1')
ax[1].set_title('1-dimensional PCA', size = 15)
plt.tight_layout()
plt.show()
```

![](/asstets/KPCA/2.png)

이처럼 기존의 PCA방법으로는 각 변수의 특징을 두드러지게 할 수 없기 때문에 $Kernel$ 방법을 사용해야만 한다.

# 2. Derivation

우리는 PCA에서 다음과 같은 식을 얻을 수 있다.

$$
\Sigma = \frac{1}{n}\sum_{i = 1}^nx_i x_i^t \Rightarrow \Sigma v = \lambda v
$$

여기에 Bernhard Scholkopf(1997)에 의해 독립변수 $x$에 비선형결합을 실시한다.

$$
\Sigma = \frac{1}{n}\sum_{i = 1}^n \phi(x_i)\phi(x_i)^t \Rightarrow \Sigma v = \lambda v
$$

위와 같은 식을 유도할 수 있으며 이 방법이 실제 PCA를 구하는 방식과 유사한지 확인해보도록 한다.

$$
\begin{split}
&\frac{1}{n}\sum_{i = 1}^n\phi(x_i)\phi(x_i)^tv = \frac{1}{n}\sum_{i = 1}^n\phi(x_i \cdot v) \phi(x_i)^t = \lambda v\\
\Leftrightarrow & \frac{1}{\lambda n}\sum_{i = 1}^n \phi(x_i) \phi(x_i)^t = v \\
\Rightarrow & \sum_{i = 1}^n a_i \phi(x_i)^t = v \\
\Rightarrow & \sum_{i = 1}^n a_{ji} \phi(x_i) = v_j
\end{split}
$$

이를 통해 고유벡터 $v_j$ 는 $\phi(x_i)$ 의 선형결합으로 표현될 수 있으며 고유벡터를 찾는 것은 $\alpha_i$ 를 구하는 것과 동치인 것을 확인하였다. 조금 더 전개하면

$$
\begin{split}
&\frac{1}{n}\sum_{i = 1}^n \phi(x_i)\phi(x_i)^t \left(\sum_{l = 1}^n a_{jl} \phi(x_l)\right) = \lambda_j \left(\sum_{l = 1}^n a_{jl} \phi(x_l)\right)\\
\Rightarrow &\frac{1}{n}\sum_{i = 1}^n\phi(x_i)\left(\sum_{l = 1}^na_{jl}K(x_i, x_l)\right) =  \lambda_j \left(\sum_{l = 1}^n a_{jl} \phi(x_l)\right)\\
\Rightarrow &\frac{1}{n}\sum_{i = 1}^n\phi(x_k)^t\phi(x_i)\left(\sum_{l = 1}^na_{jl}K(x_i, x_l)\right) = \lambda_j \left(\sum_{l = 1}^n a_{jl} \phi(x_k)\phi(x_l)\right)\\
\Rightarrow & \sum_{i = 1}^n\sum_{l = 1}^na_{jl}K(x_k, x_i)K(x_i, x_l) = n\lambda_j \sum_{l = 1}^na_{jl}K(x_k, x_l)\\
& \therefore Ka_{j} = n\lambda_ja_j
\end{split}
$$

이며 또한 $v_j^tv_j = 1$은 아래와 같이 표현된다.

$$
v_j^tv_j = \sum_{k = 1}^n\sum_{l = 1}\alpha_{jl}\alpha_{jk}\phi(x_i)^t\phi(x_k) = 1 \Rightarrow \alpha_j^tK\alpha_j = 1
$$

으로 KPCA를 푸는 방식이 PCA를 푸는 방식과 다르지 않음을 보였다. 추가적으로 $\phi(x_i)$의 평균이 0을 가지지 않을 때를 위해 일반화된 form은 아래와 같다.

$\tilde{\phi}(x_k) = \phi(x_k) - \frac{1}{n}\sum_{k = 1}^n\phi(x_k)$이라 가정하면

$$
\begin{split}
\tilde{K(x_i, x_j)} &= \tilde{\phi}(x_i)^t\tilde{\phi}(x_j)\\
&= \left(\phi(x_i) - \frac{1}{n}\sum_{k = 1}^n\phi(x_k)\right)^t\left(\phi(x_j) - \frac{1}{n}\sum_{k = 1}^n\phi(x_k)\right)\\
&= \phi(x_i)^t\phi(x_j) - \frac{1}{n}\sum_{k = 1}^n\phi(x_i)^t\phi(x_k) - \frac{1}{n}\sum_{k = 1}^n\phi(x_k)^t\phi(x_j) + \frac{1}{n^2}\sum_{k = 1}^n\sum_{l = 1}^n\phi(x_l)^t\phi(x_k)\\
&= K(x_i, x_j) - \frac{1}{n}\sum_{k = 1}^nK(x_i, x_k) - \frac{1}{n}\sum_{k = 1}^nK(x_j, x_k) + \frac{1}{n^2}\sum_{k = 1}^n\sum_{l = 1}^nK(x_l, x_k)\\
&\therefore \tilde{K} =  K - 2\textbf{1}_{1/n}K + \textbf{1}_{1/n}K\textbf{1}_{1/n}
\end{split}
$$

# 3. Examples

## 3-1. Circle data

먼저 다음의 데이터를 살펴보자.

```python
X, y = make_circles(n_samples = 200, random_state = 1, noise = 0.1, factor = 0.2)
plt.scatter(X[y == 0, 0], X[y == 0, 1], color='red', marker='^', alpha=0.5)
plt.scatter(X[y == 1, 0], X[y == 1, 1], color='blue', marker='o', alpha=0.5)
plt.tight_layout()
plt.title('True plot', size = 15)
plt.show()
```

기존의 PCA 방법을 이용해 로지스틱회귀를 실시하면 다음의 결과를 얻게 된다.

```python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=1)
from sklearn.preprocessing import StandardScaler

stdsc = StandardScaler()
X_train_std= stdsc.fit_transform(X_train)
X_test_std = stdsc.fit_transform(X_test)

pca1 = PCA(n_components = 2)
pca2 = PCA(n_components = 1)
X_train_pca1= pca1.fit_transform(X_train_std)
X_test_pca1 = pca1.transform(X_test_std)
X_train_pca2= pca2.fit_transform(X_train_std)
X_test_pca2 = pca2.transform(X_test_std)

fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))
ax[0].scatter(X_train_pca1[y_train == 0, 0], X_train_pca1[y_train == 0, 1], color='red', marker='^', alpha=0.5)
ax[0].scatter(X_train_pca1[y_train == 1, 0], X_train_pca1[y_train == 1, 1], color='blue', marker='o', alpha=0.5)
ax[1].scatter(X_train_pca2[y_train == 0, 0], np.zeros((len(X_train_pca2[y_train == 0, 0]), 1)), color='red', marker='^', alpha=0.5)
ax[1].scatter(X_train_pca2[y_train == 1, 0], np.zeros((len(X_train_pca2[y_train == 1, 0]), 1)), color='blue', marker='o', alpha=0.5)
ax[0].set_title("2-dimensional PCA", fontsize = 15)
ax[0].set_xlabel("PC 1")
ax[0].set_ylabel("PC 2")
ax[1].set_title("1-dimensional PCA", fontsize = 15)
ax[1].set_xlabel("PC 1")
```

![](/asstets/KPCA/2.png)

```python
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
pca_lr1 = lr.fit(X_train_pca1, y_train)
pca_lr1_train_acc= pca_lr1.score(X_train_pca1, y_train)
pca_lr1_test_acc = pca_lr1.score(X_test_pca1, y_test)
grid1 = []
for x2 in np.linspace(-2, 2, 500):
    for x1 in np.linspace(-2, 2, 500):
        grid1.append((x1, x2))
p_vals1 = pca_lr1.predict(grid1)
pca_lr2 = lr.fit(X_train_pca2, y_train)
pca_lr2_train_acc= pca_lr2.score(X_train_pca2, y_train)
pca_lr2_test_acc = pca_lr2.score(X_test_pca2, y_test)
grid2 = []
for x1 in np.linspace(-2, 2, 500):
    grid2.append(x1)
p_vals2 = pca_lr2.predict(np.reshape(grid2, (-1, 1)))

fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))
ax[0].scatter(X_train_pca1[y_train == 0, 0], X_train_pca1[y_train == 0, 1], color='red', marker='^', alpha=0.5)
ax[0].scatter(X_train_pca1[y_train == 1, 0], X_train_pca1[y_train == 1, 1], color='blue', marker='o', alpha=0.5)
ax[1].scatter(X_train_pca2[y_train == 0, 0], np.zeros((len(X_train_pca2[y_train == 0, 0]), 1)), color='red', marker='^', alpha=0.5)
ax[1].scatter(X_train_pca2[y_train == 1, 0], np.zeros((len(X_train_pca2[y_train == 1, 0]), 1)), color='blue', marker='o', alpha=0.5)
ax[0].set_title("2-dimensional PCA", fontsize = 15)
ax[0].set_xlabel("PC 1")
ax[0].set_ylabel("PC 2")
ax[1].set_title("1-dimensional PCA", fontsize = 15)
ax[1].set_xlabel("PC 1")

p_vals1 = np.where(np.sign(p_vals1) < 0, 0, np.sign(p_vals1))
p_vals1 = p_vals1.reshape((500, 500))
p_vals2 = np.where(np.sign(p_vals2) < 0, 0, np.sign(p_vals2))
p_vals2 = p_vals2.reshape((1, 500))
ax[0].imshow(p_vals1, origin = "lower", extent = (-2, 2, -2, 2),cmap = plt.cm.gray_r, alpha = 0.2)
ax[1].imshow(p_vals2, origin = "lower", extent = (-2, 2, -2, 2),cmap = plt.cm.gray_r, alpha = 0.2)
```
![](/asstets/KPCA/3.png)

```python
print('Training acc for pca1 logistic:', pca_lr1_train_acc)
print('Testing acc for pca1 logistic:', pca_lr1_test_acc)
print('Training acc for pca2 logistic:', pca_lr2_train_acc)
print('Testing acc for pca2 logistic:', pca_lr2_test_acc)

# Training acc for pca1 logistic: 0.5625
# Testing acc for pca1 logistic: 0.25
# Training acc for pca2 logistic: 0.5125
# Testing acc for pca2 logistic: 0.3
```

조금 더 성능을 높이고자 KPCA를 실시한다.

```python
from sklearn.decomposition import KernelPCA

kpca1 = KernelPCA(n_components=2, kernel = "rbf", gamma = 2)
kpca2 = KernelPCA(n_components=1, kernel = "rbf", gamma = 2)
X_train_kpca1= kpca1.fit_transform(X_train_std)
X_test_kpca1 = kpca1.transform(X_test_std)
X_train_kpca2= kpca2.fit_transform(X_train_std)
X_test_kpca2 = kpca2.transform(X_test_std)

fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))
ax[0].scatter(X_train_kpca1[y_train == 0, 0], X_train_kpca1[y_train == 0, 1], color='red', marker='^', alpha=0.5)
ax[0].scatter(X_train_kpca1[y_train == 1, 0], X_train_kpca1[y_train == 1, 1], color='blue', marker='o', alpha=0.5)
ax[1].scatter(X_train_kpca2[y_train == 0, 0], np.zeros((len(X_train_kpca2[y_train == 0, 0]), 1)), color='red', marker='^', alpha=0.5)
ax[1].scatter(X_train_kpca2[y_train == 1, 0], np.zeros((len(X_train_kpca2[y_train == 1, 0]), 1)), color='blue', marker='o', alpha=0.5)
ax[0].set_title("2-dimensional KPCA", fontsize = 15)
ax[0].set_xlabel("PC 1")
ax[0].set_ylabel("PC 2")
ax[1].set_title("1-dimensional KPCA", fontsize = 15)
ax[1].set_xlabel("PC 1")
```
![](/asstets/KPCA/4.png)

```python
kpca_lr1 = lr.fit(X_train_kpca1, y_train)
kpca_lr1_train_acc= kpca_lr1.score(X_train_kpca1, y_train)
kpca_lr1_test_acc = kpca_lr1.score(X_test_kpca1, y_test)
grid3 = []
for x2 in np.linspace(-2, 2, 500):
    for x1 in np.linspace(-2, 2, 500):
        grid3.append((x1, x2))
kpca1.transform(grid3)
p_vals3 = kpca_lr1.predict(grid3)
kpca_lr2 = lr.fit(X_train_kpca2, y_train)
kpca_lr2_train_acc= kpca_lr2.score(X_train_kpca2, y_train)
kpca_lr2_test_acc = kpca_lr2.score(X_test_kpca2, y_test)
grid4 = []
for x1 in np.linspace(-2, 2, 500):
    grid4.append(x1)
p_vals4 = kpca_lr2.predict(np.reshape(grid2, (-1, 1)))

fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))
ax[0].scatter(X_train_kpca1[y_train == 0, 0], X_train_kpca1[y_train == 0, 1], color='red', marker='^', alpha=0.5)
ax[0].scatter(X_train_kpca1[y_train == 1, 0], X_train_kpca1[y_train == 1, 1], color='blue', marker='o', alpha=0.5)
ax[1].scatter(X_train_kpca2[y_train == 0, 0], np.zeros((len(X_train_kpca2[y_train == 0, 0]), 1)), color='red', marker='^', alpha=0.5)
ax[1].scatter(X_train_kpca2[y_train == 1, 0], np.zeros((len(X_train_kpca2[y_train == 1, 0]), 1)), color='blue', marker='o', alpha=0.5)
ax[0].set_title("2-dimensional KPCA", fontsize = 15)
ax[0].set_xlabel("PC 1")
ax[0].set_ylabel("PC 2")
ax[1].set_title("1-dimensional KPCA", fontsize = 15)
ax[1].set_xlabel("PC 1")

p_vals3 = np.where(np.sign(p_vals3) < 0, 0, np.sign(p_vals3))
p_vals3 = p_vals3.reshape((500, 500))
p_vals4 = np.where(np.sign(p_vals4) < 0, 0, np.sign(p_vals4))
p_vals4 = p_vals4.reshape((1, 500))
ax[0].imshow(p_vals3, origin = "lower", extent = (-2, 2, -2, 2),cmap = plt.cm.gray_r, alpha = 0.2)
ax[1].imshow(p_vals4, origin = "lower", extent = (-0.5, 0.5, -0.5, 0.5),cmap = plt.cm.gray_r, alpha = 0.2)
```

![](/asstets/KPCA/5.png)

```python
print('Training acc for kpca1 logistic:', kpca_lr1_train_acc)
print('Testing acc for kpca1 logistic:', kpca_lr1_test_acc)
print('Training acc for kpca2 logistic:', kpca_lr2_train_acc)
print('Testing acc for kpca2 logistic:', kpca_lr2_test_acc)

# Training acc for kpca1 logistic: 1.0
# Testing acc for kpca1 logistic: 0.925
# Training acc for kpca2 logistic: 1.0
# Testing acc for kpca2 logistic: 0.925
```

성능이 높아졌으며 독립변수 또한 완벽한 선형분류가 가능하도록 변경되었다.

## 3-2. Moon data

```python
from sklearn.datasets import make_moons

X, y = make_moons(n_samples = 200, random_state = 1)
plt.scatter(X[y == 0, 0], X[y == 0, 1], color='red', marker='^', alpha=0.5)
plt.scatter(X[y == 1, 0], X[y == 1, 1], color='blue', marker='o', alpha=0.5)
plt.tight_layout()
plt.title('True plot', size = 15)
plt.show()
```
![](/asstets/KPCA/6.png)

기존의 PCA 방법을 이용해 로지스틱회귀를 실시하면 다음의 결과를 얻게 된다.

```python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=1)
from sklearn.preprocessing import StandardScaler

stdsc = StandardScaler()
X_train_std= stdsc.fit_transform(X_train)
X_test_std = stdsc.fit_transform(X_test)
pca1 = PCA(n_components = 2)
pca2 = PCA(n_components = 1)
X_train_pca1= pca1.fit_transform(X_train_std)
X_test_pca1 = pca1.transform(X_test_std)
X_train_pca2= pca2.fit_transform(X_train_std)
X_test_pca2 = pca2.transform(X_test_std)

fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))
ax[0].scatter(X_train_pca1[y_train == 0, 0], X_train_pca1[y_train == 0, 1], color='red', marker='^', alpha=0.5)
ax[0].scatter(X_train_pca1[y_train == 1, 0], X_train_pca1[y_train == 1, 1], color='blue', marker='o', alpha=0.5)
ax[1].scatter(X_train_pca2[y_train == 0, 0], np.zeros((len(X_train_pca2[y_train == 0, 0]), 1)), color='red', marker='^', alpha=0.5)
ax[1].scatter(X_train_pca2[y_train == 1, 0], np.zeros((len(X_train_pca2[y_train == 1, 0]), 1)), color='blue', marker='o', alpha=0.5)
ax[0].set_title("2-dimensional PCA", fontsize = 15)
ax[0].set_xlabel("PC 1")
ax[0].set_ylabel("PC 2")
ax[1].set_title("1-dimensional PCA", fontsize = 15)
ax[1].set_xlabel("PC 1")
```

![](/asstets/KPCA/7.png)

```python
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
pca_lr1 = lr.fit(X_train_pca1, y_train)
pca_lr1_train_acc= pca_lr1.score(X_train_pca1, y_train)
pca_lr1_test_acc = pca_lr1.score(X_test_pca1, y_test)
grid1 = []
for x2 in np.linspace(-2, 2, 500):
    for x1 in np.linspace(-2, 2, 500):
        grid1.append((x1, x2))
pca1.transform(grid1)
p_vals1 = pca_lr1.predict(grid1)
pca_lr2 = lr.fit(X_train_pca2, y_train)
pca_lr2_train_acc= pca_lr2.score(X_train_pca2, y_train)
pca_lr2_test_acc = pca_lr2.score(X_test_pca2, y_test)
grid2 = []
for x1 in np.linspace(-2, 2, 500):
    grid2.append(x1)
p_vals2 = pca_lr2.predict(np.reshape(grid2, (-1, 1)))

fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))
ax[0].scatter(X_train_pca1[y_train == 0, 0], X_train_pca1[y_train == 0, 1], color='red', marker='^', alpha=0.5)
ax[0].scatter(X_train_pca1[y_train == 1, 0], X_train_pca1[y_train == 1, 1], color='blue', marker='o', alpha=0.5)
ax[1].scatter(X_train_pca2[y_train == 0, 0], np.zeros((len(X_train_pca2[y_train == 0, 0]), 1)), color='red', marker='^', alpha=0.5)
ax[1].scatter(X_train_pca2[y_train == 1, 0], np.zeros((len(X_train_pca2[y_train == 1, 0]), 1)), color='blue', marker='o', alpha=0.5)
ax[0].set_title("2-dimensional PCA", fontsize = 15)
ax[0].set_xlabel("PC 1")
ax[0].set_ylabel("PC 2")
ax[1].set_title("1-dimensional PCA", fontsize = 15)
ax[1].set_xlabel("PC 1")

p_vals1 = np.where(np.sign(p_vals1) < 0, 0, np.sign(p_vals1))
p_vals1 = p_vals1.reshape((500, 500))
p_vals2 = np.where(np.sign(p_vals2) < 0, 0, np.sign(p_vals2))
p_vals2 = p_vals2.reshape((1, 500))
ax[0].imshow(p_vals1, origin = "lower", extent = (-2, 2, -2, 2),cmap = plt.cm.gray_r, alpha = 0.2)
ax[1].imshow(p_vals2, origin = "lower", extent = (-2, 2, -2, 2),cmap = plt.cm.gray_r, alpha = 0.2)
```

![](/asstets/KPCA/8.png)

```python
print('Training acc for pca1 logistic:', pca_lr1_train_acc)
print('Testing acc for pca1 logistic:', pca_lr1_test_acc)
print('Training acc for pca2 logistic:', pca_lr2_train_acc)
print('Testing acc for pca2 logistic:', pca_lr2_test_acc)

# Training acc for pca1 logistic: 0.88125
# Testing acc for pca1 logistic: 0.9
# Training acc for pca2 logistic: 0.84375
# Testing acc for pca2 logistic: 0.775
```

성능을 높이고자 KPCA를 실시한다.

```python
from sklearn.decomposition import KernelPCA

kpca1 = KernelPCA(n_components=2, kernel = "rbf", gamma = 5)
kpca2 = KernelPCA(n_components=1, kernel = "rbf", gamma = 5)
X_train_kpca1= kpca1.fit_transform(X_train_std)
X_test_kpca1 = kpca1.transform(X_test_std)
X_train_kpca2= kpca2.fit_transform(X_train_std)
X_test_kpca2 = kpca2.transform(X_test_std)

fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))
ax[0].scatter(X_train_kpca1[y_train == 0, 0], X_train_kpca1[y_train == 0, 1], color='red', marker='^', alpha=0.5)
ax[0].scatter(X_train_kpca1[y_train == 1, 0], X_train_kpca1[y_train == 1, 1], color='blue', marker='o', alpha=0.5)
ax[1].scatter(X_train_kpca2[y_train == 0, 0], np.zeros((len(X_train_kpca2[y_train == 0, 0]), 1)), color='red', marker='^', alpha=0.5)
ax[1].scatter(X_train_kpca2[y_train == 1, 0], np.zeros((len(X_train_kpca2[y_train == 1, 0]), 1)), color='blue', marker='o', alpha=0.5)
ax[0].set_title("2-dimensional KPCA", fontsize = 15)
ax[0].set_xlabel("PC 1")
ax[0].set_ylabel("PC 2")
ax[1].set_title("1-dimensional KPCA", fontsize = 15)
ax[1].set_xlabel("PC 1")
```

![](/asstets/KPCA/9.png)

```python
kpca_lr1 = lr.fit(X_train_kpca1, y_train)
kpca_lr1_train_acc= kpca_lr1.score(X_train_kpca1, y_train)
kpca_lr1_test_acc = kpca_lr1.score(X_test_kpca1, y_test)
grid3 = []
for x2 in np.linspace(-2, 2, 500):
    for x1 in np.linspace(-2, 2, 500):
        grid3.append((x1, x2))
kpca1.transform(grid3)
p_vals3 = kpca_lr1.predict(grid3)
kpca_lr2 = lr.fit(X_train_kpca2, y_train)
kpca_lr2_train_acc= kpca_lr2.score(X_train_kpca2, y_train)
kpca_lr2_test_acc = kpca_lr2.score(X_test_kpca2, y_test)
grid4 = []
for x1 in np.linspace(-2, 2, 500):
    grid4.append(x1)
grid4 = np.reshape(grid4, (-1, 1))
p_vals4 = kpca_lr2.predict(grid4)

fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))
ax[0].scatter(X_train_kpca1[y_train == 0, 0], X_train_kpca1[y_train == 0, 1], color='red', marker='^', alpha=0.5)
ax[0].scatter(X_train_kpca1[y_train == 1, 0], X_train_kpca1[y_train == 1, 1], color='blue', marker='o', alpha=0.5)
ax[1].scatter(X_train_kpca2[y_train == 0, 0], np.zeros((len(X_train_kpca2[y_train == 0, 0]), 1)), color='red', marker='^', alpha=0.5)
ax[1].scatter(X_train_kpca2[y_train == 1, 0], np.zeros((len(X_train_kpca2[y_train == 1, 0]), 1)), color='blue', marker='o', alpha=0.5)
ax[0].set_title("2-dimensional KPCA", fontsize = 15)
ax[0].set_xlabel("PC 1")
ax[0].set_ylabel("PC 2")
ax[1].set_title("1-dimensional KPCA", fontsize = 15)
ax[1].set_xlabel("PC 1")

p_vals3 = np.where(np.sign(p_vals3) < 0, 0, np.sign(p_vals3))
p_vals3 = p_vals3.reshape((500, 500))
p_vals4 = np.where(np.sign(p_vals4) < 0, 0, np.sign(p_vals4))
p_vals4 = p_vals4.reshape((1, 500))
ax[0].imshow(p_vals3, origin = "lower", extent = (-0.7, 0.7, -0.7, 0.7),cmap = plt.cm.gray_r, alpha = 0.2)
ax[1].imshow(p_vals4, origin = "lower", extent = (-0.5, 0.5, -0.5, 0.5),cmap = plt.cm.gray_r, alpha = 0.2)
```

![](/asstets/KPCA/10.png)

```python
print('Training acc for kpca1 logistic:', kpca_lr1_train_acc)
print('Testing acc for kpca1 logistic:', kpca_lr1_test_acc)
print('Training acc for kpca2 logistic:', kpca_lr2_train_acc)
print('Testing acc for kpca2 logistic:', kpca_lr2_test_acc)

# Training acc for kpca1 logistic: 0.9
# Testing acc for kpca1 logistic: 0.95
# Training acc for kpca2 logistic: 0.9
# Testing acc for kpca2 logistic: 0.95
```

성능이 높아졌으며 독립변수 또한 선형분리가 가능하게 변경되었다.
