---
layout: post
title:  "Auto-encoder"
date:   2019-03-01
use_math: true
tags:
 - Python
 - english
 - Research
---

# Introduction

Yichuan Tang(2013) and Shi-Xiong et el.(2015) have done learning minimizes a margind-based loss instead of the cross-entropy loss and changing softmax as support vector machine(SVM) at image data and voice data respectively. Also they have done multiclass SVM but just fitting original SVM in this paper.

# Basic concept

## SVM

original SVM assume notations that

* ture labels : $t_n \in \{-1, 1\}, \quad n = 1, \ldots, N$
* given training data : $x_n, \quad n = 1, \ldots, N$
* penalize data point which violate the margin requirements : $\epsilon_n, \quad n = 1, \ldots, N$
* corresponding weight : $w$

then our optimization L2-SVM loss is the following

$$
\begin{split}
\frac{1}{2}w^Tw + C\sum_{n = 1}^N \max(1 - w^T x_n \cdot t_n, 0)^2
\end{split}
$$

then predicted class label of a test data is

$$
\begin{split}
sign(w^Tx_n)
\end{split}
$$

## Softmax

When use obove notations, optimize cross-entropy loss in this neural network(same as logistic regression in this paper).

$$
\begin{split}
-\sum_{n = 1}^N\big[ t_n\cdot \log \frac{\exp (w^Tx_n)}{1 + \exp (w^Tx_n)} + (1 - t_n)\cdot \log \big(1 - \frac{1}{1 + \exp (w^Tx_n)}\big) \big]
\end{split}
$$

then predicted class label of a test data is

$$
\begin{split}
\max \big( \frac{\exp(w^Tx_n)}{1 + \exp(w^Tx_n)}, \frac{1}{1 + \exp(w^Tx_n)} \big)
\end{split}
$$

# Simulation

## Linear classification

```python
%reset -f

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from numpy.random import multivariate_normal
from pandas import DataFrame

np.random.seed(1)

n0, mu0, var0 = 500, [10, 11], 10
data0 = multivariate_normal(mu0, np.eye(2)*var0, n0)
df0 = DataFrame(data0, columns = ['x1', 'x2'])
df0["t"] = -1

n1, mu1, var1 = 400, [18, 20], 10
data1 = multivariate_normal(mu1, np.eye(2)*var1, n1)
df1 = DataFrame(data1, columns = ['x1', 'x2'])
df1["t"] = 1

df = pd.concat([df0, df1], ignore_index = True)

fig = plt.figure(figsize = (6, 6))
subplot = fig.add_subplot(1, 1, 1)
subplot.set_ylim([0, 30])
subplot.set_xlim([0, 30])
subplot.scatter(df0["x1"], df0["x2"], marker = "x")
subplot.scatter(df1["x1"], df1["x2"], marker = "o")
subplot.set_title("True plot", fontsize = 20)
plt.savefig('1.png', dpi=300)
```

![](/assets/DLSVM/1.png)

## Original SVM

```python
import tensorflow as tf
from sklearn.model_selection import train_test_split 
X_train,X_test,t_train,t_test= np.array(train_test_split(df[["x1","x2"]],
                                                         df.iloc[:,2:3],random_state=0))
                                                         
num_factors = 1
num_features = 2
num_units1 = 20

X = tf.placeholder(tf.float32, shape=[None, num_features])
w1 = tf.Variable(tf.truncated_normal([num_features, num_factors]))
b1 = tf.Variable(tf.zeros([num_factors]))
C = tf.constant([0.1])

t = tf.placeholder(tf.float32, [None, num_factors])
hinge = tf.maximum(1 - tf.multiply(tf.matmul(X, w1) + b1, t), 0)
cost = tf.reduce_sum(tf.square(w1))
loss = tf.add(cost, tf.multiply(C, tf.reduce_sum(tf.square(hinge))))
train_step = tf.train.AdamOptimizer().minimize(loss)

p = tf.matmul(X, w1) + b1
correct_prediction = tf.equal(tf.sign(p), tf.sign(t))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))                                                         
np.random.seed(1)
tf.set_random_seed(1)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

train_accuracy1= []
test_accuracy1 = []

i = 0

for _ in range(10001):
    i += 1
    _, loss_val = sess.run([train_step, loss], 
                           feed_dict = {X: X_train, t: np.array(t_train)})
    if i % 1000 == 0 or i == 1:
        train_acc = sess.run(accuracy, 
                             feed_dict = {X: X_train, t: np.array(t_train)})
        train_accuracy1.append(train_acc)
        test_acc = sess.run(accuracy, 
                            feed_dict = {X: X_test, t: np.array(t_test)})
        test_accuracy1.append(test_acc)
        print('step:%d,loss:%f,Train:%f,Test:%f' 
              %(i,loss_val,train_acc,test_acc))

# step:1,loss:11907.932617,Train:0.545185,Test:0.560000
# step:1000,loss:797.524536,Train:0.487407,Test:0.515556
# step:2000,loss:600.258484,Train:0.500741,Test:0.515556
# step:3000,loss:372.327087,Train:0.511111,Test:0.524444
# step:4000,loss:172.339737,Train:0.564444,Test:0.564444
# step:5000,loss:57.708321,Train:0.693333,Test:0.742222
# step:6000,loss:27.001005,Train:0.954074,Test:0.982222
# step:7000,loss:19.776754,Train:0.968889,Test:0.986667
# step:8000,loss:15.196682,Train:0.973333,Test:0.986667
# step:9000,loss:12.206654,Train:0.973333,Test:0.991111
# step:10000,loss:10.239092,Train:0.973333,Test:0.991111

grid1 = []
for x2 in np.linspace(0, 30, 500):
    for x1 in np.linspace(0, 30, 500):
        grid1.append((x1, x2))
p_vals1 = sess.run(p, feed_dict = {X: grid1})
p_vals1 = np.where(np.sign(p_vals1) < 0, 0, np.sign(p_vals1))
p_vals1 = p_vals1.reshape((500, 500))

fig = plt.figure(figsize = (6, 6))
subplot = fig.add_subplot(1, 1, 1)
subplot.set_ylim([0, 30])
subplot.set_xlim([0, 30])
subplot.scatter(df0["x1"], df0["x2"], marker = "x")
subplot.scatter(df1["x1"], df1["x2"], marker = "o")
subplot.set_title("Predict SVM", fontsize = 20)
subplot.imshow(p_vals1, origin = "lower", extent = (0, 30, 0, 30),
              cmap = plt.cm.gray_r, alpha = 0.2)
plt.savefig('2.png', dpi=300)
```

![](/assets/DLSVM/2.png)

Linear SVM only split with linear line.

## Neural network

```python
test_t = np.array(pd.get_dummies(t_test.astype("category")))
train_t= np.array(pd.get_dummies(t_train.astype("category")))

num_factors = 2
num_features = 2
num_units1 = 20

X = tf.placeholder(tf.float32, shape=[None, num_features])
w1 = tf.Variable(tf.truncated_normal([num_features, num_units1]))
b1 = tf.Variable(tf.zeros([num_units1]))
hidden1 = tf.nn.tanh(tf.matmul(X, w1) + b1)

w2 = tf.Variable(tf.truncated_normal([num_units1, num_factors]))
b2 = tf.Variable(tf.zeros([num_factors]))
p = tf.nn.softmax(tf.matmul(hidden1, w2) + b2)

t = tf.placeholder(tf.float32, [None, num_factors])
loss = -tf.reduce_sum(t*tf.log(p))
train_step = tf.train.AdamOptimizer().minimize(loss)

correct_prediction = tf.equal(tf.sign(p-0.5), tf.sign(t-0.5))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

np.random.seed(1)
tf.set_random_seed(1)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

train_accuracy2= []
test_accuracy2 = []

i = 0

for _ in range(10001):
    i += 1
    _, loss_val = sess.run([train_step, loss], 
                           feed_dict = {X: X_train, t: train_t})
    if i % 1000 == 0 or i == 1:
        train_acc = sess.run(accuracy, feed_dict = {X: X_train, t: train_t})
        train_accuracy2.append(train_acc)
        test_acc = sess.run(accuracy, feed_dict = {X: X_test, t: test_t})
        test_accuracy2.append(test_acc)
        print('step:%d,loss:%f,Train:%f,Test:%f' 
              %(i,loss_val,train_acc,test_acc))

# step:1,loss:550.978394,Train:0.434074,Test:0.471111
# step:1000,loss:54.355011,Train:0.973333,Test:0.982222
# step:2000,loss:48.289291,Train:0.974815,Test:0.982222
# step:3000,loss:47.590656,Train:0.974815,Test:0.982222
# step:4000,loss:47.235016,Train:0.974815,Test:0.986667
# step:5000,loss:46.880611,Train:0.974815,Test:0.986667
# step:6000,loss:46.592869,Train:0.974815,Test:0.982222
# step:7000,loss:46.234661,Train:0.973333,Test:0.982222
# step:8000,loss:43.909763,Train:0.976296,Test:0.982222
# step:9000,loss:42.266827,Train:0.976296,Test:0.982222
# step:10000,loss:41.064789,Train:0.976296,Test:0.982222

grid2 = []
for x2 in np.linspace(0, 30, 500):
    for x1 in np.linspace(0, 30, 500):
        grid2.append((x1, x2))
p_vals2 = sess.run(p, feed_dict = {X: grid2})
p_vals2 = np.argmax(p_vals2, axis = 1)
p_vals2 = p_vals2.reshape((500, 500))

fig = plt.figure(figsize = (6, 6))
subplot = fig.add_subplot(1, 1, 1)
subplot.set_ylim([0, 30])
subplot.set_xlim([0, 30])
subplot.scatter(df0["x1"], df0["x2"], marker = "x")
subplot.scatter(df1["x1"], df1["x2"], marker = "o")
subplot.set_title("Predict NN", fontsize = 20)
subplot.imshow(p_vals2, origin = "lower", extent = (0, 30, 0, 30),
              cmap = plt.cm.gray_r, alpha = 0.2)
plt.savefig('3.png', dpi=300)
```

![](/assets/DLSVM/3.png)

Neural network split data with smooth line.

## DLSVM

```python
num_factors = 1
num_features = 2
num_units1 = 20

X = tf.placeholder(tf.float32, shape=[None, num_features])
w1 = tf.Variable(tf.truncated_normal([num_features, num_units1]))
b1 = tf.Variable(tf.zeros([num_units1]))
hidden1 = tf.nn.relu(tf.matmul(X, w1) + b1)

w2 = tf.Variable(tf.truncated_normal([num_units1, num_factors]))
b2 = tf.Variable(tf.zeros([num_factors]))
C = tf.constant([0.1])

t = tf.placeholder(tf.float32, [None, num_factors])
hinge = tf.maximum(1 - tf.multiply(tf.matmul(hidden1, w2) + b2, t), 0)
cost = tf.add(tf.reduce_sum(tf.square(w2)), tf.reduce_sum(tf.square(w1)))
loss = tf.add(cost, tf.multiply(C, tf.reduce_sum(tf.square(hinge))))
train_step = tf.train.AdamOptimizer().minimize(loss)

p = tf.matmul(hidden1, w2) + b2
correct_prediction = tf.equal(tf.sign(p), tf.sign(t))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

np.random.seed(1)
tf.set_random_seed(1)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

train_accuracy3= []
test_accuracy3 = []

i = 0

for _ in range(10001):
    i += 1
    _, loss_val = sess.run([train_step, loss], 
                           feed_dict = {X: X_train, t: np.array(t_train)})
    if i % 1000 == 0 or i == 1:
        train_acc = sess.run(accuracy, 
                             feed_dict = {X: X_train, t: np.array(t_train)})
        train_accuracy3.append(train_acc)
        test_acc = sess.run(accuracy, 
                            feed_dict = {X: X_test, t: np.array(t_test)})
        test_accuracy3.append(test_acc)
        print('step:%d,loss:%f,Train:%f,Test:%f' 
              %(i,loss_val,train_acc,test_acc))

# step:1,loss:26135.662109,Train:0.551111,Test:0.560000
# step:1000,loss:70.754036,Train:0.866667,Test:0.893333
# step:2000,loss:45.456520,Train:0.955556,Test:0.968889
# step:3000,loss:37.314884,Train:0.968889,Test:0.986667
# step:4000,loss:34.547062,Train:0.970370,Test:0.991111
# step:5000,loss:32.796520,Train:0.971852,Test:0.991111
# step:6000,loss:31.482924,Train:0.973333,Test:0.986667
# step:7000,loss:30.302145,Train:0.974815,Test:0.986667
# step:8000,loss:28.892120,Train:0.974815,Test:0.986667
# step:9000,loss:26.993803,Train:0.974815,Test:0.986667
# step:10000,loss:24.409208,Train:0.974815,Test:0.986667

grid3 = []
for x2 in np.linspace(0, 30, 500):
    for x1 in np.linspace(0, 30, 500):
        grid3.append((x1, x2))
p_vals3 = sess.run(p, feed_dict = {X: grid3})
p_vals3 = np.where(np.sign(p_vals3) < 0, -1, np.sign(p_vals3))
p_vals3 = p_vals3.reshape((500, 500))

fig = plt.figure(figsize = (6, 6))
subplot = fig.add_subplot(1, 1, 1)
subplot.set_ylim([0, 30])
subplot.set_xlim([0, 30])
subplot.scatter(df0["x1"], df0["x2"], marker = "x")
subplot.scatter(df1["x1"], df1["x2"], marker = "o")
subplot.set_title("Predict DLSVM", fontsize = 20)
subplot.imshow(p_vals3, origin = "lower", extent = (0, 30, 0, 30),
              cmap = plt.cm.gray_r, alpha = 0.2)
plt.savefig('4.png', dpi=300)
```

![](/assets/DLSVM/4.png)

DlSVM split data with smooth and linear line.

```python
fig = plt.figure(figsize = (15, 5))
ax1 = fig.add_subplot(1, 4, 1)
ax2 = fig.add_subplot(1, 4, 2)
ax3 = fig.add_subplot(1, 4, 3)
ax1.scatter(df0["x1"], df0["x2"], marker = "x")
ax1.scatter(df1["x1"], df1["x2"], marker = "o")
ax1.imshow(p_vals1, origin = "lower", extent = (0, 30, 0, 30),
           cmap = plt.cm.gray_r, alpha = 0.2)
ax1.set_title("SVM", fontsize = 15)
ax2.scatter(df0["x1"], df0["x2"], marker = "x")
ax2.scatter(df1["x1"], df1["x2"], marker = "o")
ax2.imshow(p_vals2, origin = "lower", extent = (0, 30, 0, 30),
           cmap = plt.cm.gray_r, alpha = 0.2)
ax2.set_title("NN", fontsize = 15)
ax3.scatter(df0["x1"], df0["x2"], marker = "x")
ax3.scatter(df1["x1"], df1["x2"], marker = "o")
ax3.imshow(p_vals3, origin = "lower", extent = (0, 30, 0, 30),
           cmap = plt.cm.gray_r, alpha = 0.2)
ax3.set_title("DLSVM", fontsize = 15)
plt.savefig('5.png', dpi=300)
```

![](/assets/DLSVM/5.png)
