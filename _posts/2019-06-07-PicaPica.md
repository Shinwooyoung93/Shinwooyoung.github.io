---
layout: post
title:  "Multivariate & Data mining team project"
date:   2019-06-07
use_math: true
tags:
 - R
 - korean
 - Team project
---

# 0. Data generation

먼저 데이터의 시각화를 위해 데이터의 형태를 살펴보자.

```r
rm(list = ls())

library(pracma)
library(kernlab)
library(sparsepca)
set.seed(2)

n <- 200
p <- 1
x36   <- matrix(rnorm(n*p), n, p)
clx9  <- matrix(round(ifft(as.vector(x36)),2),nrow = n,ncol = p)
clx10 <- matrix(round(fft(as.vector(x36)),2),nrow = n,ncol = p)
X <- data.frame(Re(clx9),Im(clx9),Mod(clx9),Arg(clx9),Re(clx10),Im(clx10),Mod(clx10),Arg(clx10))
error <- rnorm(n)
y <- factor(sign(Re(clx10)*Im(clx10)+error))
table(y)
data <- data.frame(y, X)
pal <- c("#d91e02", "#0209d9")
plot(data[,-1], col = pal[data[,1]], main = "Raw data")
```
```r
![](/assets/Multivariate & Data mining - team project/ppt1.png)
```

DFT(Discrete Fourier Transform)을 이용하여 출력된 데이터의 실수부분과 허수부분이 직교한다는 사실을 이용하여 비선형 분리로 이루어진 데이터이다.

사용한 라이브러리는 

- `library(flexsurv)` : Practical Numerical Math Function의 줄임말로 고속푸리에 변환과 역변환 함수를 이용해 고차원데이터를 만드는 데 사용

- `library(kernlab)` : 비선형적 구조를 가진 데이터를 선형으로 변환하는 방법들을 이용한 통계적 기법의 함수들을 제공

- `library(sparsepca)` : PCA기법과 변수선택 모형의 조합인 SparsePCA 관련 함수를 제공

우리의 최종 목적은 `error`부분을 변경해가면서 단순 로지스틱 회귀분석만으로도 예측 정확도를 높이는 데 목적이 있다. 

이를 위해 2가지를 비교할 것이다.

- `PCA` : 고차원의 데이터를 단순히 `PCA`만으로 차원 축소 후 로지스틱

- `Kernel PCA` + `Sparse PCA` : 고차원의 데이터를 `Kernel PCA`로 차원을 축소 후, `Sparse PCA`로 다시 차원축소 후 로지스틱

# 1. Modeling

이번 절에서는 사용할 방법론들에 대해 간단하게 설명한다.

## 1-1. PCA

`n`개의 관측치와 `p`개의 변수로 구성된 데이터를 상관관계가 없는 `k`개의 변수로 구성된 데이터로 요약하는 기법이다.

요약된 변수는 기존 변수의 선형조합으로 생성된다.

## 1-2. Kernel PCA

`Kernel PCA`는 선형분리가 불가능한 비선형 데이터에 대해서 `Kernel trick`을 사용하여 선형분리가 가능하게 만들어 차원축소하는 기법이다.

이를 위해서 `PCA`에서 사용하는 공분산 행렬을

$$
\frac{1}{n}X X^t \Rightarrow \frac{1}{n}\phi(X)\phi(X)^t
$$

위의 식으로 나타내 고유벡터, 고유값을 찾아낸다.

$$
\begin{split}
&\Sigma = \frac{1}{n}\sum_{i = 1}^n\phi(x_i)\phi(x_i)^t \\ \Rightarrow& \Sigma v = \lambda v = \frac{1}{n}\sum_{i = 1}^n\phi(x_i)\phi(x_i)^tv = \frac{1}{n}\sum_{i = 1}^n\phi(x_i \cdot v)\phi(x_i)^t = \lambda v \\
\Leftrightarrow& \frac{1}{\lambda n}\sum_{i = 1}^n \phi(x_i\cdot v) \phi(x_i)^t = v \\
\Rightarrow&  \sum_{i = 1}^n \alpha_i \phi(x_i)^t = v \\
\Rightarrow&  \sum_{i = 1}^n \alpha_{ji} \phi(x_i)^t = v_j
\end{split}
$$

조금 더 자세히 전개하면

$$
\begin{split}
&\frac{1}{n}\sum_{i = 1}^n \phi(x_i)\phi(x_i)^t\left(\sum_{l = 1}^n \alpha_{jl}\phi(x_l)\right) = \lambda_j\left(\sum_{l = 1}^n\alpha_{jl}\phi(x_l)\right) \\
\Rightarrow & \frac{1}{n}\sum_{i = 1}^n \phi(x_i)\left(\sum_{l = 1}^n \alpha_{jl}\phi(x_i)^t\phi(x_l)\right) = \frac{1}{n}\sum_{i = 1}^n \phi(x_i)\left(\sum_{l = 1}^n\alpha_{jl}K(x_i, x_l)\right) = \lambda_j\left(\sum_{l = 1}^n\alpha_{jl}\phi(x_l)\right) \\
\Rightarrow & \frac{1}{n}\sum_{i = 1}^n\phi(x_k)^t\phi(x_i)\left(\sum_{l = 1}^n\alpha_{jl}K(x_i, x_l)\right) = \lambda_j\left(\sum_{l = 1}^n\alpha_{jl}\phi(x_k)^t\phi(x_l)\right) \\
\Rightarrow & \sum_{i = 1}^n\sum_{l = 1}^n\alpha_{jl}K(x_k, x_i)K(x_i, x_l) = n\lambda_j\sum_{l = 1}^nK(x_k, x_l) \quad \therefore \: K\alpha_j = n\lambda_j\alpha_j \\
\Rightarrow & v_j^tv_j = \sum_{k = 1}^n\sum_{l = 1}^n\alpha_{jl}\alpha_{jk}\phi(x_l)^t\phi(x_k) = 1 \Rightarrow \alpha_j K \alpha_j = 1
\end{split}
$$

계산된 $\alpha_j$를 찾는 것이 `Kernel PCA`의 목적이다.

간단한 예시로 다음의 데이터를 사용하였을 때, 

```{r, echo=FALSE, out.width = '50%', fig.align='center'}
knitr::include_graphics("1.png")
```

선형분리가 불가능한 데이터이다. 이 데이터에 대해 `PCA`를 사용하면

```{r, echo=FALSE, out.width = '60%', fig.align='center'}
knitr::include_graphics("2.png")
```

다음과 같이 나타나 차원축소의 의미가 없다. 이를 해결하고자 `Kernel PCA`를 사용하면

```{r, echo=FALSE, out.width = '70%', fig.align='center'}
knitr::include_graphics("4.png")
```

과 같이 선형분리를 할 수 있게 된다.

## 1-3. Sparse PCA

`Sparse PCA`는 기존 `PCA`의 Regularized 버전으로 주성분이 변수들의 선형결합형태로 이루어져 있다는 사실을 이용하여 개발된 방법론이다.

로딩벡터(Loading vector) $\alpha$에 패널티를 주어 중요한 주성분만을 선택하는 변수선택 + 변수축소 기법이다.

$$
\hat{\alpha}^{elastic} = \min_\alpha \sum_{k = 1}^p||z_k - \alpha_kx_k||^2 + \lambda_1||\alpha||_2^2 + \lambda_2||\alpha||_1
$$

로 표현된다.

# 2. Simple example

이번 절에서는 위의 8개 변수에 대한 차원축소를 진행한다.

## 2-1. PCA

단순 `PCA`만으로 차원축소를 진행한다.

시각화를 위해 2차원으로 축소를 진행한 결과는 아래와 같다.

```{r, fig.align="center", error = F, warning=F, out.width="70%"}
pca.fit <- prcomp(~., data = data[,-1])
pca.data<- data.frame(data$y, pca.fit$x[,1:2])
plot(pca.data[,-1], col = pal[pca.data[,1]], lwd = 1, main = "PCA")
```

이며 설명력은 

```{r, fig.align="center", error = F, warning=F, out.width="70%"}
plot(pca.fit$sdev^2, type = "b", col = "dodgerblue", main = "Screeplot PCA", xlab = "PC", ylab = "variance")
text(pca.fit$sdev^2, labels = round(cumsum(pca.fit$sdev^2)/sum(pca.fit$sdev^2), 3), pos = 1)
```

80%를 설명하지만 차원 축소를 진행하여도 의미가 없음을 알 수 있다.

`Loading vector plot`은 아래와 같다.

```{r, fig.align="center", error = F, warning=F, out.width="70%"}
pca.loading <- pca.fit$rotation
plot(pca.loading[,1:2], main = "Loading plot")
text(pca.loading[,1:2], labels = rownames(pca.loading))
```

## 2-2. Kernel PCA

이번에는 `Kernel PCA`만으로 차원축소를 진행한다.

사용한 `Kernel`의 종류는 `polynomial(k = 5)`이며 시각화를 위해 2차원으로 축소한다.

$$
K(x, y) = <\phi(x), \phi(y)> = \left(\sum_{i = 1}^nx_iy_i + c\right)^5
$$

```{r, fig.align="center", error = F, warning=F, out.width="70%"}
kpca.fit <- kpca(~., data = data[,-1], kernel = "polydot",
                 kpar = list(degree = 5))
kpca.data<- data.frame(data$y, pcv(kpca.fit)[,1:2])
plot(kpca.data[,-1], col = pal[kpca.data[,1]], lwd = 1,
     main = "Kernel PCA", xlab = "PC1", ylab = "PC2")
```

이며 설명력은

```{r, fig.align="center", error = F, warning=F, out.width="70%"}
plot((kpca.fit@eig^2)[1:20], type = "b", col = "dodgerblue", main = "Screeplot KPCA", xlab = "PC", ylab = "variance")
text((kpca.fit@eig^2)[1:20], labels = round(cumsum((kpca.fit@eig^2)[1:20])/sum(kpca.fit@eig^2), 3), pos = 1)
```

98%를 넘지만 `polynomial(k = 5)`만을 이용해 전체를 축소하였기 때문에 `polynomial(k = 5)`로 선형분리가 되지 않는 데이터 때문에 성능이 높지 못하다.

따라서 `Kernel PCA`를 부분적으로 사용하는 방법을 생각하였다.

## 2-3. Sparse PCA

```{r, fig.align="center", error = F, warning=F, out.width="70%"}
spca.fit <- spca(data[,-1], verbose = F)
spca.fit
spca.data<- data.frame(data$y, spca.fit$scores[,1:2])
plot(spca.data[,-1], col = pal[spca.data[,1]], 
     main = "Sparse PCA", xlab = "PC1", ylab = "PC2")
```

`PCA`와 큰 차이를 보이지 않는다. 

이는 선형으로 분리될 수 없는 것을 실시한 것이기 때문에 `KPCA`를 동시에 사용할 필요성이 존재한다.

```{r, fig.align="center", error = F, warning=F, out.width="70%"}
plot(spca.fit$sdev^2, type = "b", col = "dodgerblue", main = "Screeplot SPCA", xlab = "PC", ylab = "variance")
text(spca.fit$sdev^2, labels = round(cumsum(spca.fit$sdev^2)/sum(spca.fit$sdev^2), 3), pos = 1)
```

```{r, fig.align="center", error = F, warning=F, out.width="70%"}
spca.loading <- spca.fit$loadings
plot(spca.loading, main = "Loading plot", xlab = "PC1", ylab = "PC2")
text(spca.loading, labels = rownames(pca.loading))
```

## 2-4. Kernel PCA + Sparse PCA

이번 절에서는 전체 데이터의 차원 `p`에서 `Kernel PCA`로 축소를 진행하고, 변수선택 기능이 추가된 `Sparse PCA`를 이용해 2차원으로 재축소한다.

먼저 차원을 `p/2`로 축소를 진행한다.

```{r, fig.align="center", error = F, warning=F, out.width="70%"}
kpca.data<- data.frame(data$y, pcv(kpca.fit)[,(1:(ncol(X)/2))])
plot(kpca.data[,-1], col = pal[kpca.data[,1]], lwd = 1, 
     main = "Kernel PCA with p/2")
```

그래프를 살펴보면 선형분리가 되어있는 데이터도 존재하고 선형분리가 불가능한 비선형 데이터도 존재한다.

따라서 전체 변수를 이용해 `PCA`를 진행하기보다 필요하지 않은 데이터를 선택하지 않을 필요가 보인다.

이 상태에서 `Sparse PCA`를 진행한다.

```{r, fig.align="center", error = F, warning=F, out.width="70%"}
spca.fit <- spca(kpca.data[,-1], k = 2, verbose = F)
spca.fit$loadings
```

이며 2차원으로 줄이고자 할 때 중요한 PC들이 3번째, 4번째 PC임을 확인하였다. 

이를 바탕으로 `Kernel PCA`를 이용해 축소한 3번째, 4번째 PC를 사용하여 그래프를 확인하겠다.

```{r, fig.align="center", error = F, warning=F, out.width="70%"}
index <- which(apply(spca.fit$loadings, 1, sum) >0)
final.data <- data.frame("y" = data$y, kpca.data[,index + 1])
plot(final.data[,-1], col = pal[final.data[,1]], lwd = 1, 
     xlab = "PC1", ylab = "PC2", main = "Kernel PCA + Sparse PCA")
```

이며 `bias`가 존재하지만 선형분리가 깔끔하게 이루어질 것을 기대할 수 있다.

## 2-4. Summary

3가지 그래프를 한번에 살펴보자.

```{r, fig.align="center", error = F, warning=F, out.width="90%"}
par(mfrow = c(1, 3))
plot(pca.data[,-1], col = pal[pca.data[,1]], lwd = 1, 
     main = "PCA")
plot(kpca.data[,2:3], col = pal[kpca.data[,1]], lwd = 1, 
     main = "Kernel PCA", xlab = "PC1", ylab = "PC2")
plot(final.data[,-1], col = pal[final.data[,1]], lwd = 1, 
     xlab = "PC1", ylab = "PC2", main = "Sparse PCA + Kernel PCA")
```

이며 `Kernel PCA + Sparse PCA`의 성능이 좋을 것을 기대해볼 수 있다.

# 3. High dimensional data

차원의 수를 1000으로 늘린 후, 성능을 비교해보도록 하겠다.

## 3-1. Explicate simulation

시뮬레이션 데이터를 `200 x 1000`으로 만든 후, `error`를 변경해가며 100번의 시뮬레이션을 진행한다.

또한 성능을 비교할 수 있는 척도가 필요하므로 이는 `Accuracy`를 사용한다. 

`train data`와 `test data`를 70%와 30%로 분할 한 후 선형분류기인 로지스틱 회귀분석을 실시한다.

여기서 주의해야할 점은 관측값의 갯수가 차원의 수보다 적을 경우, `kernel trick`을 사용하게 되면 `kernel matrix`가 관측값의 수를 넘지 못한다. 

따라서 전체 변수를 한번에 줄이는 것이아니라, 랜덤하게 20개의 변수씩을 선택해서 1차원 축소를 `p/20`번 실시해 효과적으로 차원을 축소한다.

## 3-2. Simulation

### 3-2-1. Raw data

1000개의 데이터를 생성하기 위해서는 다변량 정규분포에서 데이터를 생성한 후, 로지스틱변형을 실시해 확률화를 실시한다. 

그렇게 만들어진 확률을 바탕으로 베르누이 분포에서 범주를 추출한다.

```{r, fig.align="center", error = F, warning=F, out.width="70%"}
rm(list = ls())

library(MASS)
library(pracma)
library(kernlab)
library(sparsepca)
set.seed(1)

n <- 200
p <- 1000
beta.true <- c(rep(0.5,8), rep(1,8), rep(1.5,8), rep(2,8), rep(2.5,8), rep(0,960))
Mu <- rep(0, p)
rho <- 0.95
corr.exp = abs(matrix(1:p, p, p)-t(matrix(1:p, p, p)))
Sigma = rho^corr.exp
y <- rep(0, n)
X <- mvrnorm(n, Mu, Sigma)
f.true <- exp(X%*%as.vector(beta.true))/(1+exp(X%*%as.vector(beta.true)))

for(kk in 1:n){
  y[kk] <- rbinom(1, 1, f.true[kk])
}

y <- factor(sign(y))
table(y)
data <- data.frame(y, X)
pal <- c("#d91e02", "#0209d9")
plot(data[,2:9], col = pal[data[,1]])
plot(data[,994:1001], col = pal[data[,1]])
```

초반에는 상관관계가 강하며, 차원이 커질수록 상관관계가 약하다.

```{r, echo = F, fig.align="center", error = F, warning=F, out.width="70%"}
pca.function <- function(data){
  pca.fit <- prcomp(data[,-1], rank = 2, scale = TRUE, center = TRUE)
  pca.data<- data.frame("y" = data$y, pca.fit$x)
  k <- 20
  kpca.data<-matrix(NA, nrow(data), p/k*2)
  for(j in 1:(p/k)){
    temp.index<-(k*(j - 1) + 1):(k*j)
    temp.kpca <- kpca(~., data = data[,1+temp.index], kernel = "rbfdot", kpar = list(sigma = 0.1))
    kpca.data[,(2*(j - 1) + 1):(2*j)] <- pcv(temp.kpca)[,1:2]
  }
  kpca.data<- data.frame("y" = data$y, kpca.data)
  spca.fit <- spca(kpca.data[,2:11], k = 2, verbose = F, beta = 0.01, alpha = 1e-06)
  final.data <- data.frame("y" = data$y, spca.fit$scores)
  return(list(pca.data = pca.data, kpca.data = kpca.data, final.data = final.data))
}
```

### 3-2-2. Simulation

```{r, fig.align="center", error = F, warning=F, out.width="70%"}
accuracy.mat <- matrix(NA, 100, 2)
for(simul in 1:nrow(accuracy.mat)){
  set.seed(simul)
  y <- rep(0, n)
  f.true <- exp(X%*%as.vector(beta.true))/(1 + exp(X%*%as.vector(beta.true)))
  for(kk in 1:n){
    y[kk] <- rbinom(1, 1, f.true[kk])
  }
  data <- data.frame(y = factor(y), X + rnorm(n*p, n, p)/p)
  pca.fit <- pca.function(data)
  pca.data<- pca.fit$pca.data
  final.data<-pca.fit$final.data
  
  index <- 1:(0.3*nrow(X))
  pca.test <- pca.data[index,]
  pca.train<- pca.data[-index,]
  final.test <- final.data[index,]
  final.train<- final.data[-index,]
  logistic1  <- glm(y~., data = pca.train, family = "binomial")
  logistic2  <- glm(y~., data = final.train, family = "binomial")
  pred.val1  <- ifelse(predict(logistic1, new = pca.test, type = "response") < 0.5, 0, 1)
  pred.val2  <- ifelse(predict(logistic2, new = final.test, type = "response")<0.5, 0, 1)
  accuracy1  <- mean(pca.test[,1] == pred.val1)
  accuracy2  <- mean(final.test[,1] == pred.val2)
  accuracy.mat[,1] <- accuracy1
  accuracy.mat[,2] <- accuracy2
}
colnames(accuracy.mat) <- c("PCA", "K+SPCA")
apply(accuracy.mat, 2, mean)
```

## 3-3. Summary

```{r, echo = F, fig.align="center", error = F, warning=F, out.width="70%"}
kpca.data <- pca.fit$kpca.data
plot(kpca.data[,2:9], col = pal[kpca.data[,1]], main = "Kernel PCA data, 100-th simul")
```

`Kernel PCA`로 부분 차원 축소를 진행한 결과는 위의 그래프와 같다.

선형분리가 가능할 것으로 보이며, 부분 차원 축소를 진행한 결과에 대해 좋은 주성분들을 추출해야할 필요성이 있다.

그에 따라 `Sparse PCA`를 실시한 후, 로지스틱 회귀분석으로 검정한 결과는 아래와 같다.

```{r, echo = F, fig.align="center", error = F, warning=F, out.width="90%"}
par(mfrow = c(1, 2))
r <- sapply(pca.data[,-1], range, na.rm = TRUE)
xs <- seq(r[1,1], r[2,1], length.out = 100)
ys <- seq(r[1,2], r[2,2], length.out = 100)
g <- data.frame("PC1" = rep(xs, each=100), "PC2" = rep(ys, time = 100))
p <- ifelse(predict(logistic1, new = g, type = "response") < 0.5, 0, 1)
plot(pca.data[,-1], col = pal[pca.data[,1]], lwd = 1, main = "PCA")
points(g, col = pal[p + 1], pch = ".", lwd = 2)
z <- matrix(as.integer(p), nrow = 100, byrow = TRUE)
contour(xs, ys, z, add = TRUE, drawlabels = FALSE, lwd = 1)
invisible(z)

r <- sapply(final.data[,-1], range, na.rm = TRUE)
xs <- seq(r[1,1], r[2,1], length.out = 100)
ys <- seq(r[1,2], r[2,2], length.out = 100)
g <- data.frame("X1" = rep(xs, each=100), "X2" = rep(ys, time = 100))
p <- ifelse(predict(logistic2, new = g, type = "response") < 0.5, 0, 1)
plot(final.data[,-1], col = pal[final.data[,1]], lwd = 1, 
     xlab = "PC1", ylab = "PC2", main = "Kernel PCA + Sparse PCA")
points(g, col = pal[p + 1], pch = ".", lwd = 2)
z <- matrix(as.integer(p), nrow = 100, byrow = TRUE)
contour(xs, ys, z, add = TRUE, drawlabels = FALSE, lwd = 1)
invisible(z)
```

단순 `PCA`만으로 실행한 결과와는 다르게 `Kernel PCA + Sparse PCA`를 실행한 결과의 성능이 높다는 것을 쉽게 확인할 있다.

# 4. Total summary

차원이 낮을 때의 경우와 높을 때의 경우를 통해 성능을 비교하며 분석하였다.

차원이 낮을 때의 일단적인 PCA를 통한 차원축소는 큰 의미가 없는 것으로 보여줬으며, 
Kernel PCA를 통한 차원 축소를 진행하면 나쁘지 않은 결과가 나오지만 선형분리가 되지 않는 데이터가 포함되어있어 성능이 높지 못하는 점이 있다.

Kernel PCA에 변수선택 기능이 추가된 Sparse PCA를 이용하면 PCA의 비해 좋은 결과물이 나온다는 것을 알수있었다.

차원이 높을때의 경우 성능을 비교하기 위해 Accuracy를 사용하였고, train, test data로 분할화여 로지스틱 회귀분석을 실시하였다.

PCA의 경우 53%의 정확도가 나타 났으며, Kernel PCA + Sparse PCA의 경우 75%의 정확도가 나오는 것을 알수있다.

이를 통해 차원이 높을때의 경우에도 PCA의 성능보다 Kernel PCA + Sparse PCA이 놓다는 것을 알수있었다.

결과적으로 차원이 낮을때와 높을때 모두 PCA보다 Kernel PCA + Sparse PCA가 좋다라는 것을 알수있다.

# Reference





