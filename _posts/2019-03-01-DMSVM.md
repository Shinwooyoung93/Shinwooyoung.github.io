# Introduction

DMSVM process is similar with DLSVM. But difference is using states and loss. It is called one-versus-rest-approach method. 

$$
\begin{split}
\min_{w_s, \epsilon_t} \quad &\frac{1}{2}\sum_{s = 1}^N ||w_s||^2_2 + C \sum_{t = 1}^T \epsilon_t^2 \\
\text{s.t} \quad &\text{for every training frame } t = 1, \ldots, T, \\
&\text{for every competing states } \bar{s_t} \in \{ 1, \ldots, N \} : \\
& w_{s_t}^T h_t - w_{\bar{s_t}}^T h_t \ge 1 - \epsilon_t, \quad \bar{s_t} \neq s_t
\end{split}
$$

# Simulation

## Datasets

```python
%reset -f

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from numpy.random import multivariate_normal
from pandas import DataFrame

np.random.seed(1)

n1, mu1, var1 = 150, [2, 4], 15
data1 = multivariate_normal(mu1, np.eye(2)*var1, n1)
df1 = DataFrame(data1, columns = ['x1', 'x2'])
df1["t"] = 1

n2, mu2, var2 = 100, [9, 11], 10
data2 = multivariate_normal(mu2, np.eye(2)*var2, n2)
df2 = DataFrame(data2, columns = ['x1', 'x2'])
df2["t"] = 2

n3, mu3, var3 = 50, [18, 20], 15
data3 = multivariate_normal(mu3, np.eye(2)*var3, n3)
df3 = DataFrame(data3, columns = ['x1', 'x2'])
df3["t"] = 3

n4, mu4, var4 = 50, [0, 25], 20
data4 = multivariate_normal(mu4, np.eye(2)*var4, n4)
df4 = DataFrame(data4, columns = ['x1', 'x2'])
df4["t"] = 4

df = pd.concat([df1, df2, df3, df4], ignore_index = True)

fig = plt.figure(figsize = (6, 6))
subplot = fig.add_subplot(1, 1, 1)
subplot.set_ylim([min(df["x2"]) - 1, max(df["x2"]) + 1])
subplot.set_xlim([min(df["x1"]) -1 , max(df["x1"]) + 1])
subplot.scatter(df1["x1"], df1["x2"], marker = "x")
subplot.scatter(df2["x1"], df2["x2"], marker = "o")
subplot.scatter(df3["x1"], df3["x2"], marker = "p")
subplot.scatter(df4["x1"], df4["x2"], marker = "s")
subplot.set_title("True plot", fontsize = 20)
plt.savefig('1.png', dpi=300)
```

![](/assets/DMSVM/1.png)

## Linear SVM

```python
import tensorflow as tf
from sklearn.model_selection import train_test_split 
X_train,X_test,t_train,t_test= np.array(train_test_split(df[["x1","x2"]],
                                                         df.iloc[:,2:4],random_state=0))
                                                         
test_bind = pd.concat([X_train, t_train], axis = 1)
X_test1 = test_bind.loc[test_bind["t"]==1,]
X_test1 = X_test1.iloc[:,0:2]
X_test2 = test_bind.loc[test_bind["t"]==2,]
X_test2 = X_test2.iloc[:,0:2]
X_test3 = test_bind.loc[test_bind["t"]==3,]
X_test3 = X_test3.iloc[:,0:2]
X_test4 = test_bind.loc[test_bind["t"]==4,]
X_test4 = X_test4.iloc[:,0:2]

test_t1 = np.array(pd.get_dummies(t_test.astype("category")))
train_t1= np.array(pd.get_dummies(t_train.astype("category")))
test_t2 = np.where(test_t1 == 0, 1, 0)
train_t2= np.where(train_t1== 0, 1, 0)

num_factors = 4
num_features = 2

X = tf.placeholder(tf.float32, shape=[None, num_features])
w1 = tf.Variable(tf.truncated_normal([num_features, num_factors]))
b1 = tf.Variable(tf.zeros([num_factors]))
C = tf.constant([0.1])

prob = tf.matmul(X, w1) + b1
t1 = tf.placeholder(tf.float32, [None, num_factors])
t2 = tf.placeholder(tf.float32, [None, num_factors])
cost = tf.reduce_sum(tf.square(w1))
hinge1 = tf.reduce_sum(tf.multiply(prob, t1), axis = 1)
hinge2 = tf.multiply(prob, t2)
hinge2 = tf.boolean_mask(hinge2, tf.not_equal(t2, 0.), axis = 0)
hinge2 = tf.reshape(hinge2, [-1, num_factors-1])
hinge2 = tf.reduce_max(hinge2, axis = 1)
hinge = tf.maximum(1 - hinge1 + hinge2, 0)
loss = tf.add(cost, tf.multiply(C, tf.reduce_sum(hinge)))
train_step = tf.train.AdamOptimizer().minimize(loss)

output = tf.argmax(prob, axis = 1)
correct_prediction = tf.equal(output, tf.argmax(t1, axis = 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

np.random.seed(1)
tf.set_random_seed(1)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

train_accuracy1= []
test_accuracy1 = []

i = 0

for _ in range(20001):
    i += 1
    _, loss_val = sess.run([train_step, loss], 
                           feed_dict={X:X_train,t1:train_t1,t2:train_t2})
    if i % 2000 == 0 or i == 1:
        train_acc = sess.run(accuracy, 
                             feed_dict={X:X_train,t1:train_t1,t2:train_t2})
        train_accuracy1.append(train_acc)
        test_acc = sess.run(accuracy, 
                            feed_dict={X:X_test,t1:test_t1,t2:test_t2})
        test_accuracy1.append(test_acc)
        print('step:%d,loss:%f,Train:%f,Test:%f' 
              %(i,loss_val,train_acc,test_acc))
              
# step:1,loss:332.704102,Train:0.061069,Test:0.022727
# step:2000,loss:13.307251,Train:0.778626,Test:0.772727
# step:4000,loss:8.650822,Train:0.923664,Test:0.931818
# step:6000,loss:6.486482,Train:0.927481,Test:0.943182
# step:8000,loss:5.883353,Train:0.927481,Test:0.943182
# step:10000,loss:5.701717,Train:0.919847,Test:0.954545
# step:12000,loss:5.662795,Train:0.916031,Test:0.954545
# step:14000,loss:5.655035,Train:0.916031,Test:0.954545
# step:16000,loss:5.653697,Train:0.916031,Test:0.954545
# step:18000,loss:5.653281,Train:0.916031,Test:0.954545
# step:20000,loss:5.653185,Train:0.916031,Test:0.954545

grid1 = []
for x2 in np.linspace(min(df["x2"])-1,max(df["x2"])+1, 500):
    for x1 in np.linspace(min(df["x1"])-1,max(df["x1"])+1, 500):
        grid1.append((x1, x2))
out_val1 = sess.run(output, feed_dict = {X: grid1})
out_val1 = pd.get_dummies(out_val1)
out_val11= np.array(out_val1.iloc[:, 0:1]).reshape((500, 500))
out_val12= np.array(out_val1.iloc[:, 1:2]).reshape((500, 500))
out_val13= np.array(out_val1.iloc[:, 2:3]).reshape((500, 500))

fig = plt.figure(figsize = (6, 6))
subplot = fig.add_subplot(1, 1, 1)
min_val1 = min(df["x1"]) - 1
max_val1 = max(df["x1"]) + 1
min_val2 = min(df["x2"]) - 1
max_val2 = max(df["x2"]) + 1
subplot.set_ylim([min_val2, max_val2])
subplot.set_xlim([min_val1, max_val1])
subplot.scatter(X_test1["x1"], X_test1["x2"], marker = "x")
subplot.scatter(X_test2["x1"], X_test2["x2"], marker = "o")
subplot.scatter(X_test3["x1"], X_test3["x2"], marker = "p")
subplot.scatter(X_test4["x1"], X_test4["x2"], marker = "s")
subplot.set_title("Predict SVM", fontsize = 20)
subplot.imshow(out_val11, origin = "lower", 
               extent = (min_val1, max_val1, min_val2, max_val2),
               cmap = plt.cm.gray_r, alpha = 0.7)
subplot.imshow(out_val12, origin = "lower", 
               extent = (min_val1, max_val1, min_val2, max_val2),
               cmap = plt.cm.gray_r, alpha = 0.3)
subplot.imshow(out_val13, origin = "lower", 
               extent = (min_val1, max_val1, min_val2, max_val2),
               cmap = plt.cm.gray_r, alpha = 0.1)
plt.savefig('2.png', dpi=300)
```

![](/assets/DMSVM/2.png)

Linear SVM only split with linear line.

## Neural network

```python
test_t = np.array(pd.get_dummies(t_test.astype("category")))
train_t= np.array(pd.get_dummies(t_train.astype("category")))

num_factors = 4
num_features = 2
num_units1 = 20

X = tf.placeholder(tf.float32, shape=[None, num_features])
w1 = tf.Variable(tf.truncated_normal([num_features, num_units1]))
b1 = tf.Variable(tf.zeros([num_units1]))
hidden1 = tf.nn.tanh(tf.matmul(X, w1) + b1)

w2 = tf.Variable(tf.truncated_normal([num_units1, num_factors]))
b2 = tf.Variable(tf.zeros([num_factors]))
p = tf.nn.softmax(tf.matmul(hidden1, w2) + b2)

t = tf.placeholder(tf.float32, [None, num_factors])
loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=t, logits=p), axis = -1)
train_step = tf.train.AdamOptimizer().minimize(loss)

correct_prediction = tf.equal(tf.sign(p-0.5), tf.sign(t-0.5))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

np.random.seed(1)
tf.set_random_seed(1)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

train_accuracy2= []
test_accuracy2 = []

i = 0

for _ in range(20001):
    i += 1
    _, loss_val = sess.run([train_step, loss], 
                           feed_dict = {X: X_train, t: train_t})
    if i % 2000 == 0 or i == 1:
        train_acc = sess.run(accuracy, feed_dict = {X: X_train, t: train_t})
        train_accuracy2.append(train_acc)
        test_acc = sess.run(accuracy, feed_dict = {X: X_test, t: test_t})
        test_accuracy2.append(test_acc)
        print('step:%d,loss:%f,Train:%f,Test:%f' 
              %(i,loss_val,train_acc,test_acc))

# step:1,loss:1.471224,Train:0.590649,Test:0.602273
# step:2000,loss:0.951526,Train:0.902672,Test:0.883523
# step:4000,loss:0.938232,Train:0.906489,Test:0.886364
# step:6000,loss:0.934029,Train:0.906489,Test:0.880682
# step:8000,loss:0.931962,Train:0.906489,Test:0.880682
# step:10000,loss:0.921784,Train:0.912214,Test:0.886364
# step:12000,loss:0.919847,Train:0.912214,Test:0.886364
# step:14000,loss:0.919402,Train:0.912214,Test:0.875000
# step:16000,loss:0.919293,Train:0.912214,Test:0.875000
# step:18000,loss:0.915746,Train:0.914122,Test:0.875000
# step:20000,loss:0.915488,Train:0.914122,Test:0.880682

grid2 = []
for x2 in np.linspace(min(df["x2"])-1,max(df["x2"])+1, 500):
    for x1 in np.linspace(min(df["x1"])-1,max(df["x1"])+1, 500):
        grid2.append((x1, x2))
out_val2 = np.argmax(sess.run(p, feed_dict = {X: grid2}), axis = 1)
out_val2 = pd.get_dummies(out_val2)
out_val21= np.array(out_val2.iloc[:, 0:1]).reshape((500, 500))
out_val22= np.array(out_val2.iloc[:, 1:2]).reshape((500, 500))
out_val23= np.array(out_val2.iloc[:, 2:3]).reshape((500, 500))

fig = plt.figure(figsize = (6, 6))
subplot = fig.add_subplot(1, 1, 1)
min_val1 = min(df["x1"]) - 1
max_val1 = max(df["x1"]) + 1
min_val2 = min(df["x2"]) - 1
max_val2 = max(df["x2"]) + 1
subplot.set_ylim([min_val2, max_val2])
subplot.set_xlim([min_val1, max_val1])
subplot.scatter(X_test1["x1"], X_test1["x2"], marker = "x")
subplot.scatter(X_test2["x1"], X_test2["x2"], marker = "o")
subplot.scatter(X_test3["x1"], X_test3["x2"], marker = "p")
subplot.scatter(X_test4["x1"], X_test4["x2"], marker = "s")
subplot.set_title("NN", fontsize = 20)
subplot.imshow(out_val21, origin = "lower", 
               extent = (min_val1, max_val1, min_val2, max_val2),
               cmap = plt.cm.gray_r, alpha = 0.7)
subplot.imshow(out_val22, origin = "lower", 
               extent = (min_val1, max_val1, min_val2, max_val2),
               cmap = plt.cm.gray_r, alpha = 0.3)
subplot.imshow(out_val23, origin = "lower", 
               extent = (min_val1, max_val1, min_val2, max_val2),
               cmap = plt.cm.gray_r, alpha = 0.1)
plt.savefig('3.png', dpi=300)
```

![](/assets/DMSVM/3.png)

Neural network split data with smooth line. But if over-train loss, NN does not work.

## DMSVM

```python
test_t1 = np.array(pd.get_dummies(t_test.astype("category")))
train_t1= np.array(pd.get_dummies(t_train.astype("category")))
test_t2 = np.where(test_t1 == 0, 1, 0)
train_t2= np.where(train_t1== 0, 1, 0)

num_factors = 4
num_features = 2
num_units1 = 20
num_units2 = 40

X = tf.placeholder(tf.float32, shape=[None, num_features])
w1 = tf.Variable(tf.truncated_normal([num_features, num_units1]))
b1 = tf.Variable(tf.zeros([num_units1]))
hidden1 = tf.nn.relu(tf.matmul(X, w1) + b1)

w2 = tf.Variable(tf.truncated_normal([num_units1, num_units2]))
b2 = tf.Variable(tf.zeros([num_units2]))
hidden2 = tf.nn.relu(tf.matmul(hidden1, w2) + b2)

w3 = tf.Variable(tf.truncated_normal([num_units2, num_factors]))
b3 = tf.Variable(tf.zeros([num_factors]))
C = tf.constant([0.1])
prob = tf.matmul(hidden2, w3) + b3

t1 = tf.placeholder(tf.float32, [None, num_factors])
t2 = tf.placeholder(tf.float32, [None, num_factors])
cost = tf.reduce_sum(tf.square(w3))
hinge1 = tf.reduce_sum(tf.multiply(prob, t1), axis = 1)
hinge2 = tf.multiply(prob, t2)
hinge2 = tf.boolean_mask(hinge2, tf.not_equal(t2, 0.), axis = 0)
hinge2 = tf.reshape(hinge2, [-1, num_factors-1])
hinge2 = tf.reduce_max(hinge2, axis = 1)
hinge = tf.maximum(1 - hinge1 + hinge2, 0)
loss = tf.add(cost, tf.multiply(C, tf.reduce_sum(hinge)))
train_step = tf.train.AdamOptimizer().minimize(loss)

output = tf.argmax(prob, axis = 1)
correct_prediction = tf.equal(output, tf.argmax(t1, axis = 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

np.random.seed(1)
tf.set_random_seed(1)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

train_accuracy3= []
test_accuracy3 = []

i = 0

for _ in range(20001):
    i += 1
    _, loss_val = sess.run([train_step, loss], 
                           feed_dict={X:X_train,t1:train_t1,t2:train_t2})
    if i % 2000 == 0 or i == 1:
        train_acc = sess.run(accuracy, 
                             feed_dict={X:X_train,t1:train_t1,t2:train_t2})
        train_accuracy3.append(train_acc)
        test_acc = sess.run(accuracy, 
                            feed_dict={X:X_test,t1:test_t1,t2:test_t2})
        test_accuracy3.append(test_acc)
        print('step:%d,loss:%f,Train:%f,Test:%f' 
              %(i,loss_val,train_acc,test_acc))

# step:1,loss:4860.515625,Train:0.297710,Test:0.295455
# step:2000,loss:46.568977,Train:0.935115,Test:0.909091
# step:4000,loss:25.123053,Train:0.935115,Test:0.920455
# step:6000,loss:13.860248,Train:0.938931,Test:0.909091
# step:8000,loss:9.035534,Train:0.942748,Test:0.909091
# step:10000,loss:6.892312,Train:0.938931,Test:0.920455
# step:12000,loss:5.749447,Train:0.938931,Test:0.931818
# step:14000,loss:5.015749,Train:0.942748,Test:0.931818
# step:16000,loss:4.535119,Train:0.950382,Test:0.943182
# step:18000,loss:4.243645,Train:0.946565,Test:0.920455
# step:20000,loss:3.961597,Train:0.946565,Test:0.943182

grid3 = []
for x2 in np.linspace(min(df["x2"])-1,max(df["x2"])+1, 500):
    for x1 in np.linspace(min(df["x1"])-1,max(df["x1"])+1, 500):
        grid3.append((x1, x2))
out_val3 = sess.run(output, feed_dict = {X: grid3})
out_val3 = pd.get_dummies(out_val3)
out_val31= np.array(out_val3.iloc[:, 0:1]).reshape((500, 500))
out_val32= np.array(out_val3.iloc[:, 1:2]).reshape((500, 500))
out_val33= np.array(out_val3.iloc[:, 2:3]).reshape((500, 500))

fig = plt.figure(figsize = (6, 6))
subplot = fig.add_subplot(1, 1, 1)
min_val1 = min(df["x1"]) - 1
max_val1 = max(df["x1"]) + 1
min_val2 = min(df["x2"]) - 1
max_val2 = max(df["x2"]) + 1
subplot.set_ylim([min_val2, max_val2])
subplot.set_xlim([min_val1, max_val1])
subplot.scatter(X_test1["x1"], X_test1["x2"], marker = "x")
subplot.scatter(X_test2["x1"], X_test2["x2"], marker = "o")
subplot.scatter(X_test3["x1"], X_test3["x2"], marker = "p")
subplot.scatter(X_test4["x1"], X_test4["x2"], marker = "s")
subplot.set_title("Predict DMSVM", fontsize = 20)
subplot.imshow(out_val31, origin = "lower", 
               extent = (min_val1, max_val1, min_val2, max_val2),
               cmap = plt.cm.gray_r, alpha = 0.7)
subplot.imshow(out_val32, origin = "lower", 
               extent = (min_val1, max_val1, min_val2, max_val2),
               cmap = plt.cm.gray_r, alpha = 0.3)
subplot.imshow(out_val33, origin = "lower", 
               extent = (min_val1, max_val1, min_val2, max_val2),
               cmap = plt.cm.gray_r, alpha = 0.1)
plt.savefig('4.png', dpi=300)
```

![](/assets/DMSVM/4.png)

DMSVM split data with smooth and linear line.

```python
fig = plt.figure(figsize = (15, 5))
ax1 = fig.add_subplot(1, 4, 1)
ax2 = fig.add_subplot(1, 4, 2)
ax3 = fig.add_subplot(1, 4, 3)
min_val1 = min(df["x1"]) - 1
max_val1 = max(df["x1"]) + 1
min_val2 = min(df["x2"]) - 1
max_val2 = max(df["x2"]) + 1
ax1.scatter(X_test1["x1"], X_test1["x2"], marker = "x")
ax1.scatter(X_test2["x1"], X_test2["x2"], marker = "o")
ax1.scatter(X_test3["x1"], X_test3["x2"], marker = "p")
ax1.scatter(X_test4["x1"], X_test4["x2"], marker = "s")
ax1.set_title("SVM", fontsize = 15)
ax1.imshow(out_val11, origin = "lower", 
           extent = (min_val1, max_val1, min_val2, max_val2),
           cmap = plt.cm.gray_r, alpha = 0.7)
ax1.imshow(out_val12, origin = "lower", 
           extent = (min_val1, max_val1, min_val2, max_val2),
           cmap = plt.cm.gray_r, alpha = 0.3)
ax1.imshow(out_val13, origin = "lower", 
           extent = (min_val1, max_val1, min_val2, max_val2),
           cmap = plt.cm.gray_r, alpha = 0.1)
ax2.scatter(X_test1["x1"], X_test1["x2"], marker = "x")
ax2.scatter(X_test2["x1"], X_test2["x2"], marker = "o")
ax2.scatter(X_test3["x1"], X_test3["x2"], marker = "p")
ax2.scatter(X_test4["x1"], X_test4["x2"], marker = "s")
ax2.set_title("NN", fontsize = 15)
ax2.imshow(out_val21, origin = "lower", 
           extent = (min_val1, max_val1, min_val2, max_val2),
           cmap = plt.cm.gray_r, alpha = 0.7)
ax2.imshow(out_val22, origin = "lower", 
           extent = (min_val1, max_val1, min_val2, max_val2),
           cmap = plt.cm.gray_r, alpha = 0.3)
ax2.imshow(out_val23, origin = "lower", 
           extent = (min_val1, max_val1, min_val2, max_val2),
           cmap = plt.cm.gray_r, alpha = 0.1)
ax3.scatter(X_test1["x1"], X_test1["x2"], marker = "x")
ax3.scatter(X_test2["x1"], X_test2["x2"], marker = "o")
ax3.scatter(X_test3["x1"], X_test3["x2"], marker = "p")
ax3.scatter(X_test4["x1"], X_test4["x2"], marker = "s")
ax3.imshow(out_val31, origin = "lower", 
           extent = (min_val1, max_val1, min_val2, max_val2),
           cmap = plt.cm.gray_r, alpha = 0.7)
ax3.imshow(out_val32, origin = "lower", 
           extent = (min_val1, max_val1, min_val2, max_val2),
           cmap = plt.cm.gray_r, alpha = 0.3)
ax3.imshow(out_val33, origin = "lower", 
           extent = (min_val1, max_val1, min_val2, max_val2),
           cmap = plt.cm.gray_r, alpha = 0.1)
ax3.set_title("DMSVM", fontsize = 15)
plt.savefig('5.png', dpi=300)
```

![](/assets/DMSVM/5.png)
