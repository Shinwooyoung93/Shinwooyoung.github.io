---
layout: post
title:  "Bayes project - ABC"
date:   2018-11-13
use_math: true
tags:
 - R
 - korean
 - Project
---

# ABC

## 1. Intro

2018년 1학기 베이즈 수업에서 했던 프로젝트를 살펴보겠다.

### 1-1. Overview

베이즈 추정방법은 베이즈 정리에 따라 사전정보와 Likelihood를 결합해 사후분포를 만드는 데에 기인한다. 

$$
P(\theta \: | \: x) \propto P(\theta) \times L(\theta \: | \: x)
$$

그러나 이는 언제까지나 Likelihood를 구하기 쉬울 때의 경우이며 Likelihood가 모호하거나 정의하기 어려운 경우, 사후분포를 계산한다는 것은 매우 어렵다. 

따라서 복잡한 Likelihood에 대해서도 사후분포를 구할 수 있는 방법이 바로 Approximate Bayesian Computation(ABC)이다. 간단한 예제를 보자.

![Caption for the picture.](https://upload.wikimedia.org/wikipedia/commons/thumb/8/81/A_dynamic_bistable_hidden_Markov_model.svg/326px-A_dynamic_bistable_hidden_Markov_model.svg.png)[Wikipedia, 2018]

이는 전형적인 Hidden Markov Model(HMM)이다. 참인 markov model은 숨겨져 있으며 A는 B로 전이가 될 수 있고 A가 다시 A로 전이될 수 있다. 

또한 실제 우리가 관측한 A가 A인지 혹은 B인지를 판단하는 전형적인 markov model이다. 위의 모델은 실제 의학분야의 유전체에 대해 사용되기도 한다.

위의 모델은 Likelihood를 손으로 계산하기 어렵고 복잡하므로 ABC방법을 사용할 수 있는 예제가 된다. 다음은 알고리즘에 대한 설명이다.

### 1-2. Algorithm

우선 ABC방법이 왜 베이즈 추정방법에 기인하는지에 대해 간단한 수식으로 설명하면

$$
\begin{aligned}
P(\theta \in A \textrm{ and Accepted}) 
& = P(\tilde{x} = x^* \: \textrm{ and} \: \theta \in A) \\
& = \int_{\theta \in A}p(x^* | \theta)\cdot\pi(\theta) d\theta = \int_{\theta \in A}p(\theta | x^*)\cdot p(x^*) d\theta \\
& \propto \int_{\theta \in A} p(\theta | x^*) d\theta \quad \quad \quad \quad \quad \textrm{[Peter M.Lee, 2012]}
\end{aligned}
$$ 

**Algorithm 1 : ** ABC algorithm

 1. **repeat**
 2. $\quad$**for** $i$ in 1: maxiter **do**
 3. $\quad \quad$ **Generate** $\theta \sim \pi(\theta)$
 4. $\quad \quad$ **Generate** $\tilde{x}$ ~ $p(x \:| \: \theta)$
 5. $\quad \quad$ **if** $\tilde{x} = x^*$ **then**
 6. $\quad \quad \quad$ accept $\theta$.
 7. $\quad \quad$ **else** 
 8. $\quad \quad \quad$ reject $\theta$.
 9. **until** number of $\theta = K$

그러나 $x$가 연속적이거나 복잡할 경우 기각할 확률이 매우 높기 때문에 수정된 알고리즘을 사용한다. 이는 ABC reject 알고리즘이다.

**Algorithm 2 : ** ABC-REJ algorithm

 1. **repeat**
 2. $\quad$**for** $i$ in 1: maxiter **do**
 3. $\quad \quad$ **Generate** $\theta$ ~ $\pi(\theta)$
 4. $\quad \quad$ **Generate** $\tilde{x}$ ~ $p(x \:| \: \theta)$
 5. $\quad \quad$ **if** $d(T(\tilde{x}) , T(x^{*})) \leq \epsilon$ **then**
 6. $\quad \quad \quad$ accept $\theta$.
 7. $\quad \quad$ **else** 
 8. $\quad \quad \quad$ reject $\theta$.
 9. **until** number of $\theta = K$
 
연속적이거나 복잡할 경우 일치하는 case는 매우 드물기 때문에 $\tilde{x}, x^*$의 평균 혹은 관측값의 갯수와 같은 요약통계량으로 거리를 계산해 오차보다 작을 경우 채택하는 알고리즘으로 수정되었다. 다음은 복잡한 Likilhood가 아닌 간단한 함수에 대해 예제를 보이겠다.

### 1-3. Simple example

Likelihood $x_1, \cdots, x_{100}$은 $B(20, \theta)$를 따르는 이항분포이며

* Reference prior : $\theta \sim U(0, 1)$
* Conjugate prior : $\theta \sim Beta(3, 1)$

를 생각해보자. 그리고 여기에서 쓰인 요약통계량은 $\sum x_i$ 를 사용하였다. 

```r
rm(list = ls())
set.seed(1)

# set likelihood value
N <- 100
true.theta <- 0.2
n <- 20
data.x <- rbinom(N, n, true.theta)
max.iter <- 1000000
sim.theta <- NULL
sim.data <- NULL
pos.theta <- NULL
eps <- 1

# uniform prior
for(i in 1:max.iter){
  
  sim.theta <- runif(1)
  sim.data <- rbinom(N, n, sim.theta)
  if(abs((sum(sim.data) - sum(data.x))) <= eps){pos.theta <- c(pos.theta, sim.theta)}
  pos.iter <- i
  if(length(pos.theta) == 1000) break

}

theta.hat1 <- mean(pos.theta)
accept.rate1 <- 1000/pos.iter
pos.true <- rbeta(max.iter, sum(data.x) + 1, 20*100 + 1 - sum(data.x))
hist(pos.theta, breaks = 25, freq = F, main = c("Prior from Unifom dist"))
points(density(pos.true)$x, density(pos.true)$y, col = "red", type = "l", lwd = 2)
```

<center><img src="/assets/ABC/1.png"></center>

```r
max.iter <- 1000000
sim.theta <- NULL
sim.data <- NULL
pos.theta <- NULL
eps <- 1

# beta(3, 1) prior
for(i in 1:max.iter){
  
  sim.theta <- rbeta(1, 3, 1)
  sim.data <- rbinom(N, n, sim.theta)
  if(abs((sum(sim.data) - sum(data.x))) <= eps){pos.theta <- c(pos.theta, sim.theta)}
  pos.iter <- i
  if(length(pos.theta) == 1000) break
  
}

theta.hat2 <- mean(pos.theta)
accept.rate2 <- 1000/pos.iter
pos.true <- rbeta(max.iter, sum(data.x) + 3, 20*100 + 1 - sum(data.x))
hist(pos.theta, breaks = 25, freq = F, main = c("Prior from Beta dist"))
points(density(pos.true)$x, density(pos.true)$y, col = "red", type = "l", lwd = 2)
```
<center><img src="/assets/ABC/2.png"></center>

```r
post.1 <- c(theta.hat1, accept.rate1)
post.2 <- c(theta.hat2, accept.rate2)
output <- rbind(uniform = post.1, beta = post.2)
colnames(output) <- c("theta.hat", "accept.rate")
output
```

```r
## theta.hat accept.rate
## uniform 0.2022437 0.001498889
## beta 0.2026667 0.001000000
```

## 2. Complicate example

### 2-1. Hidden Markov Model(HMM)

![](https://upload.wikimedia.org/wikipedia/commons/thumb/8/81/A_dynamic_bistable_hidden_Markov_model.svg/326px-A_dynamic_bistable_hidden_Markov_model.svg.png)[Wikipedia, 2018]

다시 아까의 예제를 살펴보면, 전이되는 횟수가 4회라고 했을 때 나올 수 있는 결과는 $x^* = \{AABBA\}$이지만 이 것이 참 markov chain이라고 말할 수는 없다. 

그리고 차원의 문제로 인해 $A$를 $A$라고 판단하는 $\gamma$는 알고 있다고 가정하고 다음의 문제를 풀어보자.

### Question. 
Assume that the true $\theta$ = 0.25 and known $\gamma$ = 0.8. 

When simulated $x^* = \{AAAABAABBAAAAAABAAA\}$, the summary statistic ($T(x)$ = # of switches) is 6 and $\epsilon = 2$


| **i** | $\theta_i$ | Simulated data($\tilde{x}$) | | $T(\cdot)$ | **Distance** |  **Outcome** |
|:---:|:---:|:-------------:|:---:|:---:|:---:|:---:|
|1|0.08|$AABAAAABA\cdots $||8|2|accepted|
|2|0.68|$AABBABABA\cdots $||13|7|rejected|
|3|0.87|$BBBABBABB\cdots $||9|3|rejected|
|4|0.43|$AABAAAAAB\cdots $||6|0|accepted|
|5|0.53|$ABBBBBAAB\cdots $||9|3|rejected|

### 2-2. R-code for HMM

```r
rm(list = ls())
set.seed(1)
#simulation function
data.gen <- function(data.n, theta, lambda = 0.8){
  
  w <- 0
  data.char <- c("A", "B")
  data.num <- sample(1:2, size = 1, replace = T)
  data <- data.char[data.num]
  
  for(k in 1:data.n){
    tr <- runif(1)
    mea <- runif(1)
    if(tr < theta){
      if(mea < lambda){
        w <- w + 1
        data.num <- (data.num*2)%%3
        data <- paste0(data, data.char[data.num])
      }else{
        data <- paste0(data, data.char[data.num])
        }
    }else if(mea < lambda){
      data <- paste0(data, data.char[data.num])
    }else{
       w <- w + 1
       data.num <- (data.num*2)%%3
       data <- paste0(data, data.char[data.num])
    }
  }
  output <- list(w = w, data = data)
  return(output)
}
```

```r
# real data
data.w <- 6
data.data <- "AAAABAABBAAAAAABAAAA"
#simulation
max.iter <- 100000
pos.theta <- NULL
pos.lambda <- NULL
for(i in 1:max.iter){
  sim.theta <- runif(1)
  sim.w <- data.gen(20, sim.theta)$w
  if(abs(sim.w - data.w) <= 2){
    pos.theta <- c(pos.theta, sim.theta)
  }
}

hist(pos.theta, breaks = 50, freq = F)
points(density(pos.theta)$x, density(pos.theta)$y, col = "red", type = "l", lwd = 2)
```

<center><img src="/assets/ABC/3.png"></center>

```r
theta.hat <- mean(pos.theta)
accept.rate <- length(pos.theta)/max.iter
cbind(theta.hat = theta.hat, accept.rate = accept.rate)
```
```r
## theta.hat accept.rate
## [1,] 0.2513052 0.33089
```

## 3. Conclusion

지금까지 ABC방법과 ABC-rejected 방법을 보았다. 이에 대한 한계와 결론은 다음과 같다.

* 채택율이 굉장이 낮다(효율이 굉장히 안좋다). $\Rightarrow$ **ABC MCMC**로 해결할 수 있다.
* Dimension이 높아지면 안좋은 결과를 가져다 준다. $\Rightarrow$ **Marginal**한 계산으로 해결할 수 있다.
* 요약통계량의 선정이 매우 중요하며 이에 대한 모호성이 존재한다.
* 그럼에도 불구하고 사전분포에 대한 가정이 전혀 필요하지 않다.
* 속도가 느리지만 컴퓨팅의 기술이 좋아져서 이는 충분히 해결가능하다.
