---
layout: post
title:  "Support Vector Machine"
date:   2018-11-13
use_math: true
tags:
 - R
 - korean
 - Research
---

# SVM

## 1. Lagrange multiplier method

제약식 하에서 해를 찾는 문제에 사용되는 방법으로 간단한 예시를 살펴보자.

$$
\begin{split}
\min\limits_{x} \quad & c^Tx \\
\text{subject to} \quad & Ax = b \\
& Gx \leq h\\
\end{split}
$$

$c$ 는 $n \times 1$ 벡터, $b$ 는 $m \times 1$ 벡터, $h$ 는 $r \times 1$ 벡터이며 
이는 후에 이야기할 SVM의 문제에 있어 중요한 역할을 하며, 위의 제약식 양변에 벡터 $u$ 와 $v \geq 0$ 를 곱해준다.

$$
\begin{split}
\textrm{subject to} \quad & u^TAx = u^Tb \\
& v^TGx \leq v^Th\\
\end{split}
$$

$$
\begin{aligned} 
\min\limits_{x} \: c^Tx 
\end{aligned}
$$ 
를 해결하고 목적함수 $c^Tx$ 의 하한값을 제시하기 위해 조금 더 전개해보자.

$$
\begin{split}
u^TAx + v^TGx & \leq u^Tb + v^Th \\
(A^Tu + G^Tv)^Tx & \leq u^Tb + v^Th \\
(-A^Tu - G^Tv)^Tx & \geq -u^Tb - v^Th \\
\\
\therefore \enspace c^T  \geq  -u^T& b - v^Th
\end{split}
$$


$\min\limits_{x} \: c^Tx$ 의 문제에서 $u , \: v$ 를 찾는 dual problem으로 변경되었다. 다시 식을 바꾸면

$$
\begin{split}
\max\limits_{u, v} \quad & -u^Tb - v^Th \\
\text{subject to} \quad & -A^Tu - G^Tv = c, \: v \geq 0\\
\end{split}
$$

dual problem의 제약식을 본 문제에 대입하여 Lagrange dual function으로 바꾸어 보자.

$$
\begin{split}
L(x, u, v) &= c^Tx + u^T(Ax - b) + v^T(Gx - h) \leq c^Tx \\
\frac{\partial L}{\partial x} &= c^T + u^TA + v^TG = 0\\
\therefore \enspace &c^T = - u^TA - v^TG
\end{split}
$$

따라서 Lagrance function에서 x가 minimize 되는 Lagrange dual function은 위의 편미분 값을 대입하면 된다.

$$
\begin{split}
\min\limits_{x}L(x, u, v) &= c^Tx + u^T(Ax - b) + v^T(Gx - h) \\
&= (-u^TA - v^TG)x + u^T(Ax - b) + v^T(Gx - h) \\
&= -u^TAx - v^TGx + u^TAx - u^Tb + v^TGx - v^Th \\
&= -u^Tb - v^Th \\
&= g(u, v)
\end{split}
$$

선형결합 형태라 convex한 함수$c^Tx$ 로 부터 합성함수 형태로 변환하였으므로 여전히 $g(u, v)$ 는 convex함수이다.

## 2. KKT condition

이는 Karush-Kuhn-Tucker의 약자이며 어떠한 제약 하에서 최적화 구현에 있어 효과적인 도움을 준다. 다음의 문제를 생각해보자.

$$
\begin{split}
\min\limits_{x} \quad & {f(x)} \\
\text{subject to} \quad & h_i(x) \leq 0 , \quad i = 1, \ldots , m \\
& l_j(x) = 0 , \quad j = 1, \ldots, r\\
\end{split}
$$

이를 풀기 한 KKT condition은 4가지를 만족해야 한다.

### Stationarity

$0 \in \partial \left[f(x) + \sum_{i = 1}^{m} u_i h_i(x) + \sum_{j = 1}^{r} v_j l_j(x) \right]$

### Complementary slackness

$u_i \cdot h_i(x) = 0 \quad \forall \: i$

### Primal feasibility

$\begin{aligned} h_i(x) \leq 0, \enspace l_j(x) = 0 \quad \forall \: i, j \end{aligned}$

### Dual feasibility

$u_i \geq 0, \quad \forall \: i$

이를 활용한다면 dimension이 높아지더라도 위에 나와 있는 $u_i$와 $v_j$에 관한 문제로 변형할 수 있다.

## 3. SVM

SVM을 설명하기 위한 그래프이다. 

![](/assets/SVM/svm.jpg)

먼저, $\angle ACB$ 를 $\theta$ 라 하고 $\overline{BC} = M$ 가로축을 $x$ 라 하자. 
굵은 검은색 선을 $f(x) = \beta_0 + \beta^Tx = 0$ 라고 하면 평면의 방향벡터는 $\beta$ 이다.

$$
\begin{split}
\cos{(\pi - \theta)} = \cos{\theta} &= \frac{<\beta,\: x^* - x_0>}{||\beta|| \cdot ||x^* - x_0||} \\
M = \cos{\theta} \cdot ||x^* - x_0|| &= \frac{1}{||\beta||}(\beta^Tx - \beta^Tx_0) \\
&= \frac{1}{||\beta||}(\beta_0 + \beta^Tx) \quad (\because \: \beta_0 + \beta^Tx = 0 ) \\
&\Rightarrow \frac{y}{||\beta||}(\beta_0 + \beta^Tx) \geq M \quad (\because \: y \in \{-1, 1\})
\end{split}
$$

결국 우리가 풀고 싶은 문제는 margin들을 maximize하는 것. 즉,  $\max\limits_{\beta_0, \beta} M$ 이고 
이는 다시 $\min\limits_{\beta_0, \beta} ||\beta||_2^2, \enspace s.t \: -y_i(\beta_0 + \beta^Tx_i) + 1 \leq 0$ 의 문제와 같다.

그러나 현실의 문제에서는 평면을 $f(x)$ 로 정확히 구분하지 못한다. 따라서 $\epsilon_i \geq 0$ 이라는 penalty를 넣어준다. 
결국 우리가 풀고자 하는 문제를 다시 서술하면

$$
\begin{split}
\min \limits_{\beta_0, \beta, \epsilon_i} \frac{1}{2}\beta^T\beta + C \sum_{i = 1}^{n}\epsilon_i, \quad \epsilon_i \geq 0
\end{split}
$$

이 때의 상수 $C$ 는 margin의 폭을 결정하는 상수이다. 위의 식을 풀어보면 라그랑지안과 KKT, $y_i(\beta_0 + \beta^Tx_i) \geq 1 - \epsilon_i$ 을 이용하면

$$
\begin{split}
L(\beta_0, \beta, \epsilon_i) 
&= \frac{1}{2}\beta^T\beta + C\sum_{i = 1}^{n}\epsilon_i + \sum_{i = 1}^{n}\alpha_i(1 - y_i(\beta_0 + \beta^Tx_i) - \epsilon_i) - \sum_{i = 1}^{n}u_i\epsilon_i, \enspace \alpha_i, \: u_i \geq 0 \\
\frac{\partial L}{\partial \beta_0} &\Rightarrow \sum_{i = 1}^{n} y_i \alpha_i = 0\\
\frac{\partial L}{\partial \beta} &\Rightarrow \beta = \sum_{i = 1}^{n} y_i \alpha_i x_i\\
\frac{\partial L}{\partial \epsilon_i} &\Rightarrow \alpha_i = C - u_i\\
\alpha_i(1 - &y_i(\beta_0 + \beta^Tx_i) - \epsilon_i) = 0, \enspace u_i\epsilon_i = 0, \enspace \Leftarrow \: \beta_0 \: \text{찾는 조건}
\end{split}
$$

위의 KKT condition에 따라 목적함수를 갱신하면

$$
\begin{split}
L(\beta_0, \beta, \epsilon_i) 
&= \frac{1}{2}\sum_{i}\sum_{j}\alpha_i y_i x_i^T x_j y_j \alpha_j + C\sum_{i = 1}^{n}\epsilon_i - \sum_{i = 1}^{n}\alpha_i\epsilon_i - \sum_{i = 1}^{n}u_i\epsilon_i + \sum_{i = 1}^{n}\alpha_i - \sum_{i}\sum_{j}\alpha_i y_i x_i^T x_j y_j \alpha_j \\
&= \sum_{i = 1}^{n} \alpha_i - \frac{1}{2}\sum_{i}\sum_{j}\alpha_i y_i x_i^T x_j y_j \alpha_j \\
&= g(\alpha_i) \\
& \sum_{i = 1}^{n}\alpha_i y_i = 0, \enspace 0 \leq \alpha_i \leq C \enspace (\because \: \alpha_i = C - u_i)
\end{split}
$$

결국 이 문제는 
$\max \limits_{\alpha_1, \cdots, \alpha_n} \sum_{i = 1}^{n} \alpha_i - \frac{1}{2}\sum_{i}\sum_{j}\alpha_i y_i x_i^T x_j y_j \alpha_j = \max \limits_{\alpha} 1^T\alpha - \frac{1}{2}\alpha^TK^*\alpha$의 문제와 같다고 볼 수 있다.

## 4. R-code

먼저 선형 SVM 부터 살펴보자.

```r
rm(list = ls())
library(quadprog)

set.seed(1)

n <- 100 # sample size
p <- 2   # predictor dimension
C <- 1   # cost for relaxation

# predictor
x <- matrix(rnorm(n*p), n, p)
# true parameters
true.beta0 <- 0
true.beta <- rep(2, p)

# true desision function
fx <- true.beta0 + x %*% true.beta

# error
e <- rnorm(n)

# output label
y <- c(sign(fx + e))

# plot
plot(x[,1], x[,2], type = "n", xlab = "x1", ylab = "x2", 
     main = "Linear Support Vector Machine")
points(x[y == 1, 1], x[y == 1, 2], pch = 1, col = 2)
points(x[y != 1, 1], x[y != 1, 2], pch = 3, col = 4)

abline(a = -true.beta0/true.beta[1], b = -true.beta[1]/true.beta[2], lty = 2)
```

![](/assets/SVM/1.png)

위에서 살펴보았듯이 $\max \limits_{\alpha_1, \cdots, \alpha_n} 1^T\alpha - \frac{1}{2}\alpha^TK^*\alpha$ 는 quadratic problem이다. Kernel을 설정해주고 solve.QP나 optim을 이용해 $\alpha$에 관해 최적화를 실시한다.

```r
# Quadratic Programming for SVM
K <- (x %*% t(x)) * (outer(y, y)) + 1.0e-8 * diag(rep(1, n))

d <- rep(1, n)
A <- cbind(y, diag(n), -diag(n))
b <- c(0, rep(0, n), rep(-C, n))
obj <- solve.QP(K, d, A, b, meq = 1) # meq = 1 means alpha^Ty = 0
alpha <- obj$solution
```

![](/assets/SVM/2.png)

solve.QP는 $A^Tb \leq b_0 \: \text{제약 하에서} \: \min(-d^T b + 1/2 b^T D b)$를 구하는 library이다.

```r
# indices for sv
sv.id <- which(1.0e-4 < alpha & alpha < (C - 1.0e-4))

# beta
beta <- apply(alpha * y * x, 2, sum)

# beta0
temp <- y[sv.id] - x[sv.id,,drop = F] %*% beta
beta0 <- mean(temp)

# plot
plot(x[,1], x[,2], type = "n", xlab = "x1", ylab = "x2", 
     main = "Linear Support Vector Machine")
points(x[y == 1, 1], x[y == 1, 2], pch = 1, col = 2)
points(x[y != 1, 1], x[y != 1, 2], pch = 3, col = 4)

abline(a = -true.beta0/true.beta[1], b = -true.beta[1]/true.beta[2], lty = 2)
abline(a = -beta0/beta[2], -beta[1]/beta[2], col = 3, lwd = 2)
legend("topright", c("true", "estimated"), col = c(1,3), lty = c(2,1))
```

![](/assets/SVM/2.png)

```r
# miss classification error
hat.fx <- beta0 + (y * K) %*% alpha
tab <- table(y, sign(hat.fx))
err <- 1 - sum(diag(tab))/sum(tab)
cat(paste0("miss classification error is ", err))
```
```r
## miss classification error is 0.11
```

주의할 것은 $\beta_0$ 를 계산할 때, 주어진 조건을 만족한다면 어떠한 $\alpha_i$ 에 대해 계산해도 상관이 없다.

하지만 현실에서 선형 분류가 많지 않으므로 $K$ 를 바꾸어 가며 사용한다. 종류는 다음과 같다.

### Linear

$$
K(x_1, x_2) = x_1^Tx_2
$$

### Polynomial

$$
K(x_1, x_2) = (x_1^Tx_2 + c)^d, \quad c > 0
$$

### Sigmoid

$$
K(x_1, x_2) = \tanh \{a(x_1^Tx_2) + b\}, \quad a, b \geq 0
$$

### Gaussian

$$
K(x_1, x_2) = \exp \{ -\frac{||x_1 - x_2||_2^2}{2\sigma^2} \}, \quad \sigma \neq 0
$$


```r
rm(list = ls())
library(quadprog)

set.seed(1)

n <- 200 # sample size
p <- 2   # predictor dimension
C <- 1   # cost for relaxation

# predictor
x <- matrix(rnorm(n*p), n, p)
# true parameters
true.beta0 <- 0
true.beta <- rep(2, p)

# true desision function
fx <- log(x[,1]^2 + x[,2]^2)

# error
e <- rnorm(n)

# output label
y <- c(sign(fx + e))

par(mfrow = c(1,2))
# plot
plot(x[,1], x[,2], type = "n", xlab = "x1", ylab = "x2", main = "Kernel SVM")
points(x[y == 1, 1], x[y == 1, 2], pch = 1, col = 2)
points(x[y != 1, 1], x[y != 1, 2], pch = 3, col = 4)


# Qquadratic Programming for SVM
# radial
q <- 1/p
normx <- drop((x^2) %*% rep(1, p))
a <- x %*% t(x)
a <- (-2 * a + normx) + outer(rep(1, n), normx, "*")
K <- exp(-a * q)

K.star <- K * (outer(y, y)) + 1.0e-8 * diag(rep(1, n))

d <- rep(1, n)
A <- cbind(y, diag(n), -diag(n))
b <- c(0, rep(0, n), rep(-C, n))
obj <- solve.QP(K.star, d, A, b, meq = 1)
alpha <- obj$solution # dual solution

# indices for sv
sv.id <- which(1.0e-3 < alpha & alpha < (C - 1.0e-3))

# beta0
temp <- y[sv.id] - t(y * K[,sv.id,drop = F]) %*% alpha
beta0 <- mean(temp)

hat.fx <- beta0 + (y * K) %*% alpha

boxplot(hat.fx[y == 1], hat.fx[y != 1])
```
![](/assets/SVM/3.png)

```r
tab <- table(y, sign(hat.fx))
err <- 1 - sum(diag(tab))/sum(tab)
cat(paste0("miss classification error is ", err))
```
```r
## miss classification error is 0
```

Kernel의 형태만 변경해주면 어떠한 데이터의 형태에서도 분류할 수 있음을 알 수 있다.

