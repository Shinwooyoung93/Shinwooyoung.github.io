---
layout: post
title:  "ESL_zip"
date:   2019-03-01
use_math: true
tags:
 - Python
 - english
 - Research
---

# Introduction

The examples were obtained by scanning some actual hand-drawn digits, and then generating additional images by random horizional shifts. Five different networks were fit to the data, and showing detials about these.

* $Net-1$: No hidden layer, equivalent to multinomial logistic regression.
* $Net-2$: One hidden layer, 12 hidden units fully connected.
* $Net-3$: Two hidden layers locally connected.
* $Net-4$: Two hidden layers, locally connected with weight sharing.
* $Net-5$: Two hidden layers, locally connected, two levels of weight sharing

# Explore train, test data

```python
%reset -f

from __future__ import print_function
import warnings
warnings.filterwarnings(action='ignore')
import pandas as pd
import numpy as np
import math
import matplotlib
import matplotlib.pyplot as plt
import os
import time
os.environ["CUDA_VISIBLE_DEVICES"] = ""

import chardet

def find_encoding(fname):
    r_file = open(fname, 'rb').read()
    result = chardet.detect(r_file)
    charenc = result['encoding']
    return charenc

path = r"C:\Users\shin\Desktop\project\private\neural_network"
train= os.path.join(path, "train.csv")
test = os.path.join(path, "test.csv")

my_encoding1 = find_encoding(train)
my_encoding2 = find_encoding(test)

train= pd.read_csv(train,encoding = my_encoding1, header = None)
test = pd.read_csv(test, encoding = my_encoding2, header = None)

display(f"Training set dimension is {train.shape}", train.head())
train_t = np.array(train.iloc[:,0])
train_X = np.array(train.drop(columns = [0]))

# 'Training set dimension is (7291, 257)'
#       0	1	2	3	4	5	6	7	8	9	...	247	248	249	250	251	252	253	254	255	256
# 0	6	-1.0	-1.0	-1.0	-1.000	-1.000	-1.000	-1.000	-0.631	0.862	...	0.304	0.823	1.000	0.482	-0.474	-0.991	-1.000	-1.000	-1.000	-1.0
# 1	5	-1.0	-1.0	-1.0	-0.813	-0.671	-0.809	-0.887	-0.671	-0.853	...	-0.671	-0.671	-0.033	0.761	0.762	0.126	-0.095	-0.671	-0.828	-1.0
# 2	4	-1.0	-1.0	-1.0	-1.000	-1.000	-1.000	-1.000	-1.000	-1.000	...	-1.000	-1.000	-1.000	-0.109	1.000	-0.179	-1.000	-1.000	-1.000	-1.0
# 3	7	-1.0	-1.0	-1.0	-1.000	-1.000	-0.273	0.684	0.960	0.450	...	-0.318	1.000	0.536	-0.987	-1.000	-1.000	-1.000	-1.000	-1.000	-1.0
# 4	3	-1.0	-1.0	-1.0	-1.000	-1.000	-0.928	-0.204	0.751	0.466	...	0.466	0.639	1.000	1.000	0.791	0.439	-0.199	-0.883	-1.000	-1.0
# 5 rows × 257 columns

display(f"Test set dimension is {test.shape}", test.head())
test_t = np.array(test.iloc[:,0])
test_X = np.array(test.drop(columns = [0]))

# 'Test set dimension is (2007, 257)'
#       0	1	2	3	4	5	6	7	8	9	...	247	248	249	250	251	252	253	254	255	256
# 0	9	-1.0	-1.0	-1.0	-1.000	-1.0	-0.948	-0.561	0.148	0.384	...	-1.000	-0.908	0.430	0.622	-0.973	-1.000	-1.0	-1.0	-1.0	-1.0
# 1	6	-1.0	-1.0	-1.0	-1.000	-1.0	-1.000	-1.000	-1.000	-1.000	...	-1.000	-1.000	-1.000	-1.000	-1.000	-1.000	-1.0	-1.0	-1.0	-1.0
# 2	3	-1.0	-1.0	-1.0	-0.593	0.7	1.000	1.000	1.000	1.000	...	1.000	0.717	0.333	0.162	-0.393	-1.000	-1.0	-1.0	-1.0	-1.0
# 3	6	-1.0	-1.0	-1.0	-1.000	-1.0	-1.000	-1.000	-1.000	-1.000	...	-1.000	-1.000	-1.000	-1.000	-1.000	-1.000	-1.0	-1.0	-1.0	-1.0
# 4	6	-1.0	-1.0	-1.0	-1.000	-1.0	-1.000	-1.000	-0.858	-0.106	...	0.901	0.901	0.901	0.290	-0.369	-0.867	-1.0	-1.0	-1.0	-1.0
# 5 rows × 257 columns

from sklearn.linear_model import LogisticRegression
clf = LogisticRegression(random_state=0, solver="newton-cg",
                         multi_class='multinomial').fit(train_X, train_t)
```

I tried to fit multinomial logistic regression it simply. And that's score is below.

```python
clf.score(test_X, test_t)

# 0.9123069257598405

import matplotlib.pyplot as plt
images = train_X[0:10,:]
labels = train_t[0:10]
fig = plt.figure(figsize = (10, 5))
for c, (image, label) in enumerate(zip(images, labels)):
    subplot = fig.add_subplot(2, 5, c + 1)
    subplot.set_xticks([])
    subplot.set_yticks([])
    subplot.set_title(label)
    subplot.imshow(image.reshape(16, 16))

train_t= np.array(pd.get_dummies(train_t))
test_t = np.array(pd.get_dummies(test_t))
plt.savefig('1.png', dpi = 300)
```

![](/assets/ESL_zip/1.png)

Images shown here have been deslanted and size normalized, resulting in $16 \times 16$ grayscale images. These 256 pixel values are used as inputs to the neural network classifier, and ten output units for each of the digits $0 - 9$.

# Simulation

## Net-1

$Net-1$ is just multinomial logistic regression becasue of no hidden layer. Also starts to overfit fairly quickly, while test performance of the others.

```python
import tensorflow as tf

num_factors = 10
num_features = 256

batch_size = 1000

X = tf.placeholder(tf.float32, shape=[None, num_features])
w = tf.Variable(tf.zeros([num_features, num_factors]))
b = tf.Variable(tf.zeros([num_factors]))
f = tf.matmul(X, w) + b
p = tf.nn.softmax(f)

t = tf.placeholder(tf.float32, [None, num_factors])
loss = -tf.reduce_sum(t*tf.log(p))
train_step = tf.train.AdamOptimizer().minimize(loss)

correct_prediction = tf.equal(tf.sign(p-0.5), tf.sign(t-0.5))
#correct_prediction = tf.equal(tf.argmax(p, 1), tf.argmax(t, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
# cast : change data type as tf.float32 becuse correct_prediction is bools type

class Batch:

    def __init__(self, data):
        self._index_in_epoch = 0
        self._epochs_completed = 0
        self._data = data
        self._num_examples = data.shape[0]
        pass

    @property
    def data(self):
        return self._data

    def next_batch(self,batch_size,shuffle = True):
        start = self._index_in_epoch
        if start == 0 and self._epochs_completed == 0:
            idx = np.arange(0, self._num_examples)
            np.random.shuffle(idx)
            self._data = self.data[idx]

        # go to the next batch
        if start + batch_size > self._num_examples:
            self._epochs_completed += 1
            rest_num_examples = self._num_examples - start
            data_rest_part = self.data[start:self._num_examples]
            idx0 = np.arange(0, self._num_examples) 
            np.random.shuffle(idx0)  
            self._data = self.data[idx0] 

            start = 0
            self._index_in_epoch = batch_size - rest_num_examples 
            end =  self._index_in_epoch  
            data_new_part =  self._data[start:end]  
            return np.concatenate((data_rest_part,
                                   data_new_part),axis=0),self._epochs_completed
        else:
            self._index_in_epoch += batch_size
            end = self._index_in_epoch
            return self._data[start:end], self._epochs_completed

np.random.seed(1)
tf.set_random_seed(1)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

train_accuracy1= []
test_accuracy1 = []

i = 0
combinded_train = Batch(np.c_[train_X, train_t])
p = train_X.shape[1]

for _ in range(2001):
    i += 1
    combinded_batch, epoch = combinded_train.next_batch(batch_size)
    X_batch = combinded_batch[:,0:p]
    t_batch = combinded_batch[:,p:]
    _, loss_val = sess.run([train_step, loss], 
                           feed_dict = {X: X_batch, t: t_batch})
    if i % 200 == 0 or i == 1:
        train_acc = sess.run(accuracy, feed_dict = {X: X_batch, t: t_batch})
        train_accuracy1.append(train_acc)
        test_acc = sess.run(accuracy, feed_dict = {X: test_X, t: test_t})
        test_accuracy1.append(test_acc)
        print('step:%d,loss:%f,Train:%f,Test:%f' 
              %(i,loss_val,train_acc,test_acc))

# step:1,loss:2302.582764,Train:0.900000,Test:0.900000
# step:200,loss:296.733032,Train:0.986100,Test:0.978625
# step:400,loss:228.372711,Train:0.989100,Test:0.982611
# step:600,loss:173.178253,Train:0.990500,Test:0.983059
# step:800,loss:128.037064,Train:0.994100,Test:0.983558
# step:1000,loss:141.963074,Train:0.992400,Test:0.983856
# step:1200,loss:141.791351,Train:0.993700,Test:0.983906
# step:1400,loss:121.788406,Train:0.994700,Test:0.983807
# step:1600,loss:120.941025,Train:0.992000,Test:0.984106
# step:1800,loss:86.800728,Train:0.996000,Test:0.984155
# step:2000,loss:100.591339,Train:0.996000,Test:0.983906

fig = plt.figure(figsize = (10, 6))
subplot = fig.add_subplot(1, 1, 1)
subplot.plot(train_accuracy1,linewidth = 2, label = "train accuracy")
subplot.plot(test_accuracy1, linewidth = 2, label = "test accuracy")
subplot.legend(loc = 'lower right', fontsize = 'x-large')
subplot.set_title("Net-1", fontsize = 20)
plt.savefig('2.png', dpi = 300)
```

![](/assets/ESL_zip/2.png)

$Net-1$ links $(256 + 1)\times 10 = 2570$, and $(256 + 1)\times 10 = 2570$ weights.

## Net-2

$Net-2$ is a single hidden layer network with 12 hidden units.

```python
num_factors = 10
num_features = 256
num_units1 = 12

batch_size = 1000

# first layer
X = tf.placeholder(tf.float32, shape=[None, num_features])
w1 = tf.Variable(tf.truncated_normal([num_features, num_units1]))
b1 = tf.Variable(tf.zeros([num_units1]))
hidden1 = tf.nn.tanh(tf.matmul(X, w1) + b1)

w2 = tf.Variable(tf.truncated_normal([num_units1, num_factors]))
b2 = tf.Variable(tf.zeros([num_factors]))
p = tf.nn.softmax(tf.matmul(hidden1, w2) + b2)

t = tf.placeholder(tf.float32, [None, num_factors])
loss = -tf.reduce_sum(t*tf.log(p))
train_step = tf.train.AdamOptimizer().minimize(loss)

correct_prediction = tf.equal(tf.sign(p-0.5), tf.sign(t-0.5))
#correct_prediction = tf.equal(tf.argmax(p, 1), tf.argmax(t, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

np.random.seed(1)
tf.set_random_seed(1)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

train_accuracy2= []
test_accuracy2 = []

i = 0
combinded_train = Batch(np.c_[train_X, train_t])
p = train_X.shape[1]

for _ in range(2001):
    i += 1
    combinded_batch, epoch = combinded_train.next_batch(batch_size)
    X_batch = combinded_batch[:,0:p]
    t_batch = combinded_batch[:,p:]
    _, loss_val = sess.run([train_step, loss], 
                           feed_dict = {X: X_batch, t: t_batch})
    if i % 200 == 0 or i == 1:
        train_acc = sess.run(accuracy, feed_dict = {X: X_batch, t: t_batch})
        train_accuracy2.append(train_acc)
        test_acc = sess.run(accuracy, feed_dict = {X: test_X, t: test_t})
        test_accuracy2.append(test_acc)
        print('step:%d,loss:%f,Train:%f,Test:%f' 
              %(i,loss_val,train_acc,test_acc))

# step:1,loss:5236.398438,Train:0.850900,Test:0.850274
# step:200,loss:1987.145874,Train:0.902500,Test:0.900100
# step:400,loss:1315.317017,Train:0.927600,Test:0.921574
# step:600,loss:1109.095459,Train:0.936400,Test:0.933034
# step:800,loss:850.609741,Train:0.952200,Test:0.939960
# step:1000,loss:699.913025,Train:0.957200,Test:0.947683
# step:1200,loss:612.382874,Train:0.966100,Test:0.954160
# step:1400,loss:528.458496,Train:0.971400,Test:0.958645
# step:1600,loss:459.319885,Train:0.975000,Test:0.962531
# step:1800,loss:387.913269,Train:0.978900,Test:0.965022
# step:2000,loss:352.940887,Train:0.981600,Test:0.966517

epoch

# 274

fig = plt.figure(figsize = (10, 5))
ax1 = fig.add_subplot(1, 2, 1)
ax2 = fig.add_subplot(1, 2, 2)
ax1.plot(train_accuracy1, linewidth = 2, label = "Net-1")
ax1.plot(train_accuracy2, linewidth = 2, label = "Net-2")
ax1.legend(loc = 'lower right', fontsize = 'x-large')
ax1.set_title("Train accuracy", fontsize = 15)
ax1.set_ylim(0.84, 1)
ax2.plot(test_accuracy1, linewidth = 2, label = "Net-1")
ax2.plot(test_accuracy2, linewidth = 2, label = "Net-2")
ax2.legend(loc = 'lower right', fontsize = 'x-large')
ax2.set_title("Test accuracy", fontsize = 15)
plt.savefig('3.png', dpi = 300)
```

![](/assets/ESL_zip/3.png)

$Net-2$ links $(256 + 1)\times 12 + (12 + 1)\times 10 = 3214$, and $(256 + 1)\times 12 + (12 + 1)\times 10 = 3214$ weights.

## Net-3

$Net-3$ uses local connectivity which means that each hidden unit is connected to only a small patch of units in the layer. In the first hidden layer(an $8 \times 8$ array), each unit takes inputs from a $3 \times 3$ filter of the input layer. In the second hidden layer, inputs are from a $5 \times 5$ filter, and again units that are one unit apart have receptive fields that are two units aparts. The weights for all other connections are set to zero. Local connectivity makes each unit responsible for extracting local features from the layer, and reduces considerably the total number of weights.

```python
num_filters1 = 1
num_factors = 10
num_features = 256

batch_size = 1000

X = tf.placeholder(tf.float32, [None, num_features])
X_image = tf.reshape(X, [-1, 16, 16, 1])
W_conv1 = tf.Variable(tf.truncated_normal([3, 3, 1, num_filters1]
                                          , stddev = 0.1))
h_conv1 = tf.nn.conv2d(X_image, W_conv1, strides = [1, 1, 1, 1]
                       , padding = "SAME")
h_conv1_cutoff = tf.nn.relu(h_conv1)
h_pool1 = tf.nn.max_pool(h_conv1_cutoff, ksize = [1, 2, 2, 1], 
                         strides = [1, 2, 2, 1], padding = "SAME")

num_filters2 = 1
W_conv2 = tf.Variable(tf.truncated_normal([5, 5, num_filters1, num_filters2]
                                          , stddev = 0.1))
h_conv2 = tf.nn.conv2d(h_pool1, W_conv2, strides = [1, 1, 1, 1]
                       , padding = "SAME")
h_conv2_cutoff = tf.nn.relu(h_conv2)
h_pool2 = tf.nn.max_pool(h_conv2_cutoff, ksize = [1, 2, 2, 1], 
                         strides = [1, 2, 2, 1], padding = "SAME")

h_pool2_flat = tf.reshape(h_pool2, [-1, 4*4*num_filters2])
w = tf.Variable(tf.zeros([4*4*num_filters2, num_factors]))
b = tf.Variable(tf.zeros([num_factors]))
f = tf.matmul(h_pool2_flat, w) + b
p = tf.nn.softmax(f)

t = tf.placeholder(tf.float32, [None, num_factors])
loss = -tf.reduce_sum(t*tf.log(p))
train_step = tf.train.AdamOptimizer().minimize(loss)

correct_prediction = tf.equal(tf.sign(p-0.5), tf.sign(t-0.5))
#correct_prediction = tf.equal(tf.argmax(p, 1), tf.argmax(t, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

np.random.seed(1)
tf.set_random_seed(1)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

train_accuracy3= []
test_accuracy3 = []

i = 0
combinded_train = Batch(np.c_[train_X, train_t])
p = train_X.shape[1]

for _ in range(2001):
    i += 1
    combinded_batch, epoch = combinded_train.next_batch(batch_size)
    X_batch = combinded_batch[:,0:p]
    t_batch = combinded_batch[:,p:]
    _, loss_val = sess.run([train_step, loss], 
                           feed_dict = {X: X_batch, t: t_batch})
    if i % 200 == 0 or i == 1:
        train_acc = sess.run(accuracy, feed_dict = {X: X_batch, t: t_batch})
        train_accuracy3.append(train_acc)
        test_acc = sess.run(accuracy, feed_dict = {X: test_X, t: test_t})
        test_accuracy3.append(test_acc)
        print('step:%d,loss:%f,Train:%f,Test:%f' 
              %(i,loss_val,train_acc,test_acc))

# step:1,loss:2302.582764,Train:0.900000,Test:0.900000
# step:200,loss:1195.248535,Train:0.922900,Test:0.923418
# step:400,loss:681.864807,Train:0.957400,Test:0.949975
# step:600,loss:578.658569,Train:0.963500,Test:0.957250
# step:800,loss:502.396393,Train:0.968400,Test:0.961485
# step:1000,loss:450.050018,Train:0.971100,Test:0.963279
# step:1200,loss:414.994446,Train:0.972700,Test:0.964973
# step:1400,loss:388.064575,Train:0.973300,Test:0.966168
# step:1600,loss:359.875366,Train:0.976300,Test:0.968710
# step:1800,loss:357.753540,Train:0.975600,Test:0.970105
# step:2000,loss:330.241730,Train:0.977600,Test:0.970852

ax1.plot(train_accuracy3, linewidth = 2, label = "Net-3")
ax1.legend(loc = 'lower right', fontsize = 'x-large')
ax2.plot(test_accuracy3, linewidth = 2, label = "Net-3")
ax2.legend(loc = 'lower right', fontsize = 'x-large')
fig
fig.savefig('4.png', dpi = 300)
```

![](/assets/ESL_zip/4.png)

## Net-4

$Net-4$ have local connectivity with shared weights. All units in a local feature map perform the $same$ operation on different parts of the image. In the first hidden layer, each unit takes inputs from two $3 \times 3$ filters of the input layer and make two $8 \times 8$ images. Then in the second hidden layer, each unit takes inputs from a $5 \times 5$ filter of the input layer and make a $4 \times 4$ image. Finally fully connect second hidden units.

```python
num_filters1 = 2
num_factors = 10
num_features = 256

batch_size = 1000

X = tf.placeholder(tf.float32, [None, num_features])
X_image = tf.reshape(X, [-1, 16, 16, 1])
W_conv1 = tf.Variable(tf.truncated_normal([3, 3, 1, num_filters1]
                                          , stddev = 0.1))
h_conv1 = tf.nn.conv2d(X_image, W_conv1, strides = [1, 1, 1, 1]
                       , padding = "SAME")
h_conv1_cutoff = tf.nn.relu(h_conv1)
h_pool1 = tf.nn.max_pool(h_conv1_cutoff, ksize = [1, 2, 2, 1], 
                         strides = [1, 2, 2, 1], padding = "SAME")

num_filters2 = 1
W_conv2 = tf.Variable(tf.truncated_normal([5, 5, num_filters1, num_filters2]
                                          , stddev = 0.1))
h_conv2 = tf.nn.conv2d(h_pool1, W_conv2, strides = [1, 1, 1, 1]
                       , padding = "SAME")
h_conv2_cutoff = tf.nn.relu(h_conv2)
h_pool2 = tf.nn.max_pool(h_conv2_cutoff, ksize = [1, 2, 2, 1], 
                         strides = [1, 2, 2, 1], padding = "SAME")

h_pool2_flat = tf.reshape(h_pool2, [-1, 4*4*num_filters2])
w = tf.Variable(tf.zeros([4*4*num_filters2, num_factors]))
b = tf.Variable(tf.zeros([num_factors]))
f = tf.matmul(h_pool2_flat, w) + b
p = tf.nn.softmax(f)

t = tf.placeholder(tf.float32, [None, num_factors])
loss = -tf.reduce_sum(t*tf.log(p))
train_step = tf.train.AdamOptimizer().minimize(loss)

correct_prediction = tf.equal(tf.sign(p-0.5), tf.sign(t-0.5))
#correct_prediction = tf.equal(tf.argmax(p, 1), tf.argmax(t, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

np.random.seed(1)
tf.set_random_seed(1)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

train_accuracy4= []
test_accuracy4 = []

i = 0
combinded_train = Batch(np.c_[train_X, train_t])
p = train_X.shape[1]

for _ in range(2001):
    i += 1
    combinded_batch, epoch = combinded_train.next_batch(batch_size)
    X_batch = combinded_batch[:,0:p]
    t_batch = combinded_batch[:,p:]
    _, loss_val = sess.run([train_step, loss], 
                           feed_dict = {X: X_batch, t: t_batch})
    if i % 200 == 0 or i == 1:
        train_acc = sess.run(accuracy, feed_dict = {X: X_batch, t: t_batch})
        train_accuracy4.append(train_acc)
        test_acc = sess.run(accuracy, feed_dict = {X: test_X, t: test_t})
        test_accuracy4.append(test_acc)
        print('step:%d,loss:%f,Train:%f,Test:%f' 
              %(i,loss_val,train_acc,test_acc))

# step:1,loss:2302.582764,Train:0.900000,Test:0.900000
# step:200,loss:959.368286,Train:0.940300,Test:0.935077
# step:400,loss:566.093018,Train:0.964100,Test:0.960638
# step:600,loss:428.320038,Train:0.974600,Test:0.966168
# step:800,loss:367.992493,Train:0.975900,Test:0.968809
# step:1000,loss:316.369202,Train:0.980600,Test:0.971101
# step:1200,loss:304.598358,Train:0.982100,Test:0.972945
# step:1400,loss:279.231415,Train:0.982300,Test:0.973792
# step:1600,loss:297.380188,Train:0.983900,Test:0.974340
# step:1800,loss:250.996185,Train:0.983200,Test:0.974589
# step:2000,loss:251.366776,Train:0.985400,Test:0.975037

ax1.plot(train_accuracy4, linewidth = 2, label = "Net-4")
ax1.legend(loc = 'lower right', fontsize = 'x-large')
ax2.plot(test_accuracy4, linewidth = 2, label = "Net-4")
ax2.legend(loc = 'lower right', fontsize = 'x-large')
fig
fig.savefig('5.png', dpi = 300)
```

![](/assets/ESL_zip/5.png)

## Net-5

$Net-5$ have local connectivity with shared weights. All units in a local feature map perform the $same$ operation on different parts of the image. In the first hidden layer, each unit takes inputs from two $3 \times 3$ filters of the input layer and make two $8 \times 8$ images. Then in the second hidden layer, each unit takes inputs from four $5 \times 5$ filter of the input layer and make four $4 \times 4$ images. Finally fully connect second hidden units.

```python
num_filters1 = 2
num_factors = 10
num_features = 256

batch_size = 1000

X = tf.placeholder(tf.float32, [None, num_features])
X_image = tf.reshape(X, [-1, 16, 16, 1])
W_conv1 = tf.Variable(tf.truncated_normal([3, 3, 1, num_filters1]
                                          , stddev = 0.1))
h_conv1 = tf.nn.conv2d(X_image, W_conv1, strides = [1, 1, 1, 1]
                       , padding = "SAME")
h_conv1_cutoff = tf.nn.relu(h_conv1)
h_pool1 = tf.nn.max_pool(h_conv1_cutoff, ksize = [1, 2, 2, 1], 
                         strides = [1, 2, 2, 1], padding = "SAME")

num_filters2 = 4
W_conv2 = tf.Variable(tf.truncated_normal([5, 5, num_filters1, num_filters2]
                                          , stddev = 0.1))
h_conv2 = tf.nn.conv2d(h_pool1, W_conv2, strides = [1, 1, 1, 1]
                       , padding = "SAME")
h_conv2_cutoff = tf.nn.relu(h_conv2)
h_pool2 = tf.nn.max_pool(h_conv2_cutoff, ksize = [1, 2, 2, 1], 
                         strides = [1, 2, 2, 1], padding = "SAME")

h_pool2_flat = tf.reshape(h_pool2, [-1, 4*4*num_filters2])
w = tf.Variable(tf.zeros([4*4*num_filters2, num_factors]))
b = tf.Variable(tf.zeros([num_factors]))
f = tf.matmul(h_pool2_flat, w) + b
p = tf.nn.softmax(f)

t = tf.placeholder(tf.float32, [None, num_factors])
loss = -tf.reduce_sum(t*tf.log(p))
train_step = tf.train.AdamOptimizer().minimize(loss)

correct_prediction = tf.equal(tf.sign(p-0.5), tf.sign(t-0.5))
#correct_prediction = tf.equal(tf.argmax(p, 1), tf.argmax(t, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

np.random.seed(1)
tf.set_random_seed(1)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

train_accuracy5= []
test_accuracy5 = []

i = 0
combinded_train = Batch(np.c_[train_X, train_t])
p = train_X.shape[1]

for _ in range(2001):
    i += 1
    combinded_batch, epoch = combinded_train.next_batch(batch_size)
    X_batch = combinded_batch[:,0:p]
    t_batch = combinded_batch[:,p:]
    _, loss_val = sess.run([train_step, loss], 
                           feed_dict = {X: X_batch, t: t_batch})
    if i % 200 == 0 or i == 1:
        train_acc = sess.run(accuracy, feed_dict = {X: X_batch, t: t_batch})
        train_accuracy5.append(train_acc)
        test_acc = sess.run(accuracy, feed_dict = {X: test_X, t: test_t})
        test_accuracy5.append(test_acc)
        print('step:%d,loss:%f,Train:%f,Test:%f' 
              %(i,loss_val,train_acc,test_acc))

# step:1,loss:2302.582764,Train:0.900000,Test:0.900000
# step:200,loss:418.201660,Train:0.975900,Test:0.969258
# step:400,loss:238.119934,Train:0.984800,Test:0.979322
# step:600,loss:154.404678,Train:0.989600,Test:0.982760
# step:800,loss:129.958557,Train:0.991700,Test:0.985003
# step:1000,loss:129.197021,Train:0.993800,Test:0.985999
# step:1200,loss:110.892784,Train:0.993900,Test:0.986746
# step:1400,loss:97.778656,Train:0.995300,Test:0.986996
# step:1600,loss:71.361008,Train:0.996200,Test:0.987444
# step:1800,loss:51.887825,Train:0.997300,Test:0.987843
# step:2000,loss:61.563213,Train:0.996500,Test:0.987693

ax1.plot(train_accuracy5, linewidth = 2, label = "Net-5")
ax1.legend(loc = 'lower right', fontsize = 'x-large')
ax2.plot(test_accuracy5, linewidth = 2, label = "Net-5")
ax2.legend(loc = 'lower right', fontsize = 'x-large')
fig
fig.savefig('6.png', dpi = 300)
```

| **Network Architecture** || **Links** || **Weights** || **Test accuracy** |
|:---|-|:---:|-|:---:|-|:---:|
| Net-1: Single layer network || 2570 || 2570 || 0.983906 |
| Net-2: Two layer network || 3214 || 3214 || 0.969806 |
| Net-3: Locally connected || 1226 || 1226 || 0.974141 |
| Net-4: Constrained network1 || 2266 || 1132 || 0.975486 |
| Net-5: Constrained network2 || 5194 || 1060 || 0.986298 |

I notice neural network always vouch performance. Multinomial logistics can also provide enough performance and when we use one hidden layer, do not use relu activation function. That can be produced error.
