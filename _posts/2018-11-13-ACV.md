---
layout: post
title:  "Averaging Cross Calidation"
date:   2018-11-13
use_math: true
tags:
 - R
 - korean
 - Research
---

# Averaging K-fold CV

우리는 데이터를 만나게 되었을 때, 이 값의 true값을 알지 못한다. 그에 따라 우리가 세운 모델에 대해 검증하기 곤란한 경우가 많다. 그렇기 때문에, 갖고 있는 데이터를 k개 만큼 나누어 (k-1)개를 train set으로 만들고 1개를 test set으로 만드는 k-fold cv를 사용하곤 한다.

k-fold cv는 변수선택의 기능도 포함하는데, 이에 아이디어를 얻어 각각의 k번의 모델적합을 시킨다. k개 중에서 유의한 변수들은 계속해서 높은 회귀계수를 가질 것이고, 유의하지 않은 변수들은 낮은 회귀계수를 가질 것이라고 판단하고 averaging효과를 주는 방법이 Averaging K-fold CV이다. 이는 bagging방식과 유사하며 아래의 선형모델을 생각해보자

$$
\begin{split}
Y = X\beta + \epsilon
\end{split}
$$

이 선형모델을 놓고 ACV를 하기 위해선 몇가지 용어 정의가 필요하다.

* $n_k$ is sample size in $k^{th}, \: \text{where} \: \sum_{k = 1}^{K}n_k = n$


* $M_k$ is selected model based on the $k^{th}$ hold-out fold(test set)


* $X_k$is design matrix of correspinding design matrix with full rank for any $k$


* $X_{n_k}$is design matrix of test set, also $X_{-n_k}$is design matrix of train set


* Consequently, $Y_k$ and $Y_{-k}$ are response observations corresponding to $X_{n_k}$ and $X_{-n_k}$ respectively


* $T_k$ is transformation matrix, which is size $p \times |M_k|$ that can be used in averaging selected model parameters.

추가적인 설명을 하면 $X_{-k}$는 변수선택을 하지 않은 독립변수들이며 full rank이며 $X_{-n_k}$는 변수선택이 되어진 독립변수들이다. 그렇기때문에, $Y_{-n_k}$는 존재할 필요가 없다. 또한 $T_k$는 k마다 선택되어지는 독립변수들이 달라지기 때문에 이를 위해 선택되어지지 않은 독립변수를 0이라고 놓기 위함이다. 

간단한 예시를 들면, 만약 $x_2$와 $x_4$가 변수선택되었다 했을 때 $T_k$는 두개의 컬럼을 포함한다.

$(0, 1, 0, 0, 0)^t, (0, 0, 0, 1, 0)^t$이고 이는 선택되어진 $\beta$의 추정값이 $(1, 2)^t$라면 $T_k \hat{\beta}_k = (0, 1, 0, 2, 0)^t$이다.

결국 우리가 찾고자 하는 것은

$$
\begin{split}
\hat{\beta}_{ACV} &= \frac{1}{K}\sum_{k = 1}^{K}\hat{\beta}_k = \frac{1}{K}\sum_{k = 1}^{K}T_k(X^t_kX_k)^{-1}X^t_kY \\
\hat{Y}_{ACV} &= X\hat{\beta}_{ACV} = \frac{1}{K}\sum_{k = 1}^{K}X_k(X^t_kX_k)^{-1}X^t_kY, \enspace (\because \: T_kX = X_k)
\end{split}
$$

## R-code for CV

```r
cv.function <- function(X, y, k.fold = 10){
  
  # initializing
  av.mse <- rep(0, 2^p-1)
  a <- 0
  cv.t <- matrix(0, (2^p-1)*(p+1), p+1)
  
  for(i in 1:p){ # i is number of variable

    for(j in 1:ncol(combn(p, i))){ # j is combination of i
      
      beta.l <- matrix(NA, p + 1, k.fold)
      mse <- rep(NA, k.fold)
      
      for(c in 1:k.fold){
        
        test <- (((c-1)*length(y)/k.fold)+1):(c*length(y)/k.fold)
        
        x.test <- x[test,]
        y.test <- y[test]  
        
        x.train <- x[(-test),]
        y.train <- y[(-test)]
        
        t <- matrix(0, p + 1, i + 1); t[1, 1] <- 1 # t[1, 1] is intercept
        
        for(k in 1:length(combn(p, i)[, j])){ # transform vector
          t[combn(p, i)[k, j] + 1, k + 1] <- 1
        }
        
        beta.l[, c]<-crossprod(t(t), lm(y.train ~ x.train[, combn(p, i)[, j]])$coef)
        mse[c]<-sum((y.test-cbind(rep(1,(length(y)/k.fold)),x.test)%*%beta.l[,c])^2)
      }
      
      a <- a + cumsum(length(j))
      av.mse[a] <- mean(mse)
      cv.t[((a-1)*(p+1)+1):(a*(p+1)), 1:(i+1)] <- t 

    }
  }
  
  cv.minimum <- which.min(av.mse)
  cv.min.t <- cv.t[((cv.minimum-1)*(p+1)+1):(cv.minimum*(p+1)),]
  cv.min.x <- t(tcrossprod(cv.min.t, cbind(rep(1, length(y)), x)))[, -1]
  cv.min.beta <- lm(y ~ cv.min.x)$coef
  for(n in 1:length(cv.min.beta)){if(is.na(cv.min.beta[n])==TRUE) cv.min.beta[n]<-0}
  
  obj = list(cv.min.beta = cv.min.beta)
}
```

## R-code for ACV

```r
acv.function <- function(X, y, k.fold = 10){
  
  # initializing
  cv.minimum <- rep(NA, k.fold)
  cv.min.mse <- rep(NA, k.fold)
  cv.min.beta <- matrix(NA, p +1, k.fold)
  
  for(c in 1:k.fold){
    
    mse <- rep(0, 2^p-1)
    beta.l <- matrix(0, p+1, 2^p-1)
    a <- 0
    
    test <- (((c-1)*length(y)/k.fold)+1):(c*length(y)/k.fold)
    
    x.test <- x[test,]
    y.test <- y[test]  
    
    x.train <- x[(-test),]
    y.train <- y[(-test)]
    
    for(i in 1:p){ # i is number of variable
      
      for(j in 1:ncol(combn(p, i))){ # j is combination of i
        
        t <- matrix(0, p + 1, i + 1); t[1, 1] <- 1 # t[1, 1] is intercept
        
        for(k in 1:length(combn(p, i)[, j])){ # transform vector
          t[combn(p, i)[k, j] + 1, k + 1] <- 1}
        
        a <- a + cumsum(length(j))
        beta.l[,a]<-crossprod(t(t), lm(y.train ~ x.train[, combn(p, i)[, j]])$coef)
        mse[a]<-sum((y.test-cbind(rep(1,(length(y)/k.fold)),x.test)%*%beta.l[,a])^2)
      }
    }
    
    cv.minimum[c] <- which.min(mse)
    cv.min.mse[c] <- mse[cv.minimum[c]]
    cv.min.beta[,c] <- beta.l[,cv.minimum[c]]
    #cat(paste0(c, "-fold = ", round(cv.min.beta[,c], 4)), "\n")
  }
  obj = list(acv.min.mse = cv.min.mse, acv.min.beta = cv.min.beta)
}
```

```r
set.seed(1)
n <- 100

beta <- c(1, 2, 3, 4, 1, 2, 0)
p <- length(beta)
x <- matrix(rnorm(n*p), n, p)

sigma <- 0.5
e <- rnorm(n, 0, sigma)
y <- x %*% beta + e
X <- cbind(rep(1, length(y)), x)

obj.acv <- acv.function(x, y)
obj.cv <- cv.function(x, y)

acv.beta <- apply(obj.acv$acv.min.beta, 1, mean)
cv.beta <- obj.cv$cv.min.beta

rbind(acv.beta, cv.beta)
```
```r
## (Intercept) cv.min.x1 cv.min.x2 cv.min.x3 cv.min.x4 cv.min.x5
## acv.beta 0.0020803191 1.007004 2.027539 2.919072 4.015475 1.038268
## cv.beta -0.0001439633 1.006982 2.026843 2.918821 4.013916 1.039080
## cv.min.x6 cv.min.x7
## acv.beta 1.916854 0.01107063
## cv.beta 1.915851 0.00000000
```

```r
acv.se <- mean((y - X%*%acv.beta)^2);acv.se
```
```r
## [1] 0.279824
```


```r
cv.se <- mean((y - X%*%cv.beta)^2);cv.se
```
```r
## [1] 0.2809316
```
